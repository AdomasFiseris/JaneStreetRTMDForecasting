{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max date_id: 1698\n",
      "Cutoff date_id: 1498\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import joblib\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "\n",
    "def read_parquet_in_chunks(file_path, columns=None, chunk_size=1_000_000):\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    total_rows = parquet_file.metadata.num_rows\n",
    "    for start_row in range(0, total_rows, chunk_size):\n",
    "        end_row = min(start_row + chunk_size, total_rows)\n",
    "        batch = parquet_file.read_row_group(0, columns=columns, use_threads=True).slice(start_row, end_row - start_row)\n",
    "        yield batch.to_pandas()\n",
    "\n",
    "# Initialize max_date\n",
    "max_date = None\n",
    "file_paths = sorted(glob.glob('Data/train.parquet/*/*.parquet'))\n",
    "\n",
    "# Determine the maximum date_id across all files\n",
    "for file_path in file_paths:\n",
    "    table = pq.read_table(file_path, columns=['date_id'])\n",
    "    date_ids = table['date_id'].to_pandas()\n",
    "    file_max_date = date_ids.max()\n",
    "    if max_date is None or file_max_date > max_date:\n",
    "        max_date = file_max_date\n",
    "\n",
    "# Subtract 200 to get the cutoff date\n",
    "cutoff_date = max_date - 200\n",
    "\n",
    "print(f\"Max date_id: {max_date}\")\n",
    "print(f\"Cutoff date_id: {cutoff_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1/10: Data/train.parquet\\partition_id=0\\part-0.parquet\n",
      "Processing file 2/10: Data/train.parquet\\partition_id=1\\part-0.parquet\n",
      "Processing file 3/10: Data/train.parquet\\partition_id=2\\part-0.parquet\n",
      "Processing file 4/10: Data/train.parquet\\partition_id=3\\part-0.parquet\n",
      "Processing file 5/10: Data/train.parquet\\partition_id=4\\part-0.parquet\n",
      "Processing file 6/10: Data/train.parquet\\partition_id=5\\part-0.parquet\n",
      "Processing file 7/10: Data/train.parquet\\partition_id=6\\part-0.parquet\n",
      "Processing file 8/10: Data/train.parquet\\partition_id=7\\part-0.parquet\n",
      "Processing file 9/10: Data/train.parquet\\partition_id=8\\part-0.parquet\n",
      "Processing file 10/10: Data/train.parquet\\partition_id=9\\part-0.parquet\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LIBSVM files\n",
    "train_libsvm_file = 'train.libsvm'\n",
    "val_libsvm_file = 'val.libsvm'\n",
    "\n",
    "# Define the window size for rolling mean\n",
    "window_size = 800\n",
    "\n",
    "# Process each Parquet file\n",
    "for file_idx, file_path in enumerate(file_paths):\n",
    "    print(f\"Processing file {file_idx+1}/{len(file_paths)}: {file_path}\")\n",
    "    \n",
    "    # Read the file in chunks\n",
    "    for df_chunk in read_parquet_in_chunks(file_path, chunk_size=1_000_000):\n",
    "        \n",
    "        df_chunk = df_chunk.sort_values(['date_id', 'time_id'])\n",
    "        \n",
    "        # Ensure feature columns are float64 to avoid dtype issues\n",
    "        df_chunk = df_chunk.astype('float64')\n",
    "\n",
    "        feature_cols = [col for col in df_chunk.columns if col not in ['responder_6']]\n",
    "\n",
    "        feature_names_pickle = 'feature_columns.pkl'\n",
    "        pd.to_pickle(feature_cols, feature_names_pickle)\n",
    "        \n",
    "        # Split into train and validation based on cutoff_date\n",
    "        train_chunk = df_chunk[df_chunk['date_id'] <= cutoff_date]\n",
    "        val_chunk = df_chunk[df_chunk['date_id'] > cutoff_date]\n",
    "        \n",
    "        # Process training data\n",
    "        if not train_chunk.empty:\n",
    "            X_train = train_chunk[feature_cols]\n",
    "            y_train = train_chunk['responder_6']\n",
    "\n",
    "            # Impute NaN values using backward-looking rolling mean\n",
    "            X_train_imputed = X_train.copy()\n",
    "            for col in feature_cols:\n",
    "                # Create a mask of NaN values\n",
    "                na_mask = X_train_imputed[col].isna()\n",
    "                if na_mask.any():\n",
    "                    # Forward fill to handle initial NaNs\n",
    "                    col_ffill = X_train_imputed[col].ffill()\n",
    "                    # Compute rolling mean (looking back)\n",
    "                    col_roll_mean = col_ffill.rolling(window=window_size, min_periods=1).mean()\n",
    "                    # Ensure data types are consistent\n",
    "                    col_roll_mean = col_roll_mean.astype(X_train_imputed[col].dtype)\n",
    "                    # Fill NaNs in the original series with rolling mean values\n",
    "                    X_train_imputed.loc[na_mask, col] = col_roll_mean[na_mask].astype(X_train_imputed[col].dtype)\n",
    "                    # If there are still NaNs (e.g., at the start), fill with overall mean\n",
    "                    if X_train_imputed[col].isna().any():\n",
    "                        overall_mean = X_train_imputed[col].mean()\n",
    "                        X_train_imputed[col] = X_train_imputed[col].fillna(overall_mean)\n",
    "            # Convert target variable to float64\n",
    "            y_train = y_train.astype('float64')\n",
    "\n",
    "            # Ensure no remaining NaNs\n",
    "            X_train_imputed = X_train_imputed.fillna(0)\n",
    "            \n",
    "            # Append to train LIBSVM file\n",
    "            with open(train_libsvm_file, 'ab') as f_train:\n",
    "                dump_svmlight_file(X_train_imputed, y_train, f_train, zero_based=True)\n",
    "\n",
    "        # Process validation data\n",
    "        if not val_chunk.empty:\n",
    "            X_val = val_chunk[feature_cols]\n",
    "            y_val = val_chunk['responder_6']\n",
    "\n",
    "            # Impute NaN values using backward-looking rolling mean\n",
    "            X_val_imputed = X_val.copy()\n",
    "            for col in feature_cols:\n",
    "                # Create a mask of NaN values\n",
    "                na_mask = X_val_imputed[col].isna()\n",
    "                if na_mask.any():\n",
    "                    # Forward fill to handle initial NaNs\n",
    "                    col_ffill = X_val_imputed[col].ffill()\n",
    "                    # Compute rolling mean (looking back)\n",
    "                    col_roll_mean = col_ffill.rolling(window=window_size, min_periods=1).mean()\n",
    "                    # Ensure data types are consistent\n",
    "                    col_roll_mean = col_roll_mean.astype(X_val_imputed[col].dtype)\n",
    "                    # Fill NaNs in the original series with rolling mean values\n",
    "                    X_val_imputed.loc[na_mask, col] = col_roll_mean[na_mask].astype(X_val_imputed[col].dtype)\n",
    "                    # If there are still NaNs, fill with overall mean\n",
    "                    if X_val_imputed[col].isna().any():\n",
    "                        overall_mean = X_val_imputed[col].mean()\n",
    "                        X_val_imputed[col] = X_val_imputed[col].fillna(overall_mean)\n",
    "            # Convert target variable to float64\n",
    "            y_val = y_val.astype('float64')\n",
    "\n",
    "            # Ensure no remaining NaNs\n",
    "            X_val_imputed = X_val_imputed.fillna(0)\n",
    "            \n",
    "            # Append to validation LIBSVM file\n",
    "            with open(val_libsvm_file, 'ab') as f_val:\n",
    "                dump_svmlight_file(X_val_imputed, y_val, f_val, zero_based=True)\n",
    "        \n",
    "        # Clean up\n",
    "        del df_chunk\n",
    "        gc.collect()\n",
    "\n",
    "        # Clean up training variables\n",
    "        if 'train_chunk' in locals():\n",
    "            del train_chunk\n",
    "        if 'X_train' in locals():\n",
    "            del X_train, y_train, X_train_imputed\n",
    "\n",
    "        # Clean up validation variables\n",
    "        if 'val_chunk' in locals():\n",
    "            del val_chunk\n",
    "        if 'X_val' in locals():\n",
    "            del X_val, y_val, X_val_imputed\n",
    "\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename(train_libsvm_file, 'train.libsvm.cache')\n",
    "os.rename(val_libsvm_file, 'val.libsvm.cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/24 17:03:52 INFO mlflow.tracking.fluent: Experiment with name 'XGBoost Out-of-Core Training 5' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.99396\tvalidation-rmse:0.67732\n",
      "[10]\ttrain-rmse:0.93150\tvalidation-rmse:0.63949\n",
      "[20]\ttrain-rmse:0.85972\tvalidation-rmse:0.61806\n",
      "[30]\ttrain-rmse:0.80078\tvalidation-rmse:0.61863\n",
      "[32]\ttrain-rmse:0.78740\tvalidation-rmse:0.61907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afise\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\mlflow\\types\\utils.py:407: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "C:\\Users\\afise\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [17:04:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\c_api\\c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3833453d84c3461698c8763b21e2b89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/24 17:04:22 INFO mlflow.tracking._tracking_service.client: 🏃 View run Training with Early Stopping at: http://localhost:5000/#/experiments/9/runs/686af10ca2b44e83b4ddf8d2b3499d99.\n",
      "2024/11/24 17:04:22 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/9.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment('XGBoost Out-of-Core Training 5')\n",
    "\n",
    "params = {\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.0126,\n",
    "    'subsample': 0.6919,\n",
    "    'colsample_bytree': 0.6527,\n",
    "    'gamma': 0.3388,\n",
    "    'min_child_weight': 1,\n",
    "    'reg_alpha': 0.1218,\n",
    "    'reg_lambda': 2.7785,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': 'hist',\n",
    "    'eval_metric': 'rmse',\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "mlflow.xgboost.autolog(disable=True)\n",
    "\n",
    "with mlflow.start_run(run_name='Training with Early Stopping'):\n",
    "    \n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    dtrain = xgb.DMatrix('train.libsvm.cache?format=libsvm')\n",
    "    dval = xgb.DMatrix('val.libsvm.cache?format=libsvm')\n",
    "    \n",
    "    evals = [(dtrain, 'train'), (dval, 'validation')]\n",
    "    \n",
    "    # Train the model with early stopping\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=10\n",
    "    )\n",
    "    \n",
    "    # Log the best iteration\n",
    "    mlflow.log_metric('best_iteration', model.best_iteration)\n",
    "    \n",
    "    # Save and log the model using joblib\n",
    "    joblib.dump(model, 'model_with_early_stopping.joblib')\n",
    "    mlflow.log_artifact('model_with_early_stopping.joblib')\n",
    "\n",
    "    feature_names = pd.read_pickle('feature_columns.pkl')\n",
    "    input_example = pd.DataFrame([[0]*len(feature_names)], columns=feature_names)\n",
    "    \n",
    "    # Log the model\n",
    "    mlflow.xgboost.log_model(model, artifact_path='model', input_example=input_example)\n",
    "    \n",
    "    # Log feature importance\n",
    "    importance = model.get_score(importance_type='gain')\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': list(importance.keys()),\n",
    "        'importance': list(importance.values())\n",
    "    })\n",
    "    importance_df.to_csv('feature_importance.csv', index=False)\n",
    "    mlflow.log_artifact('feature_importance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of boosting rounds: 24\n"
     ]
    }
   ],
   "source": [
    "best_iteration = model.best_iteration\n",
    "# XGBoost uses 0-based indexing, so add 1 to get the total number of boosting rounds\n",
    "optimal_num_boost_round = best_iteration + 1\n",
    "print(f\"Optimal number of boosting rounds: {optimal_num_boost_round}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
