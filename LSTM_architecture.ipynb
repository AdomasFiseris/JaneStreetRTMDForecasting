{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import gc\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def polars_to_pandas(polars_df):\n",
    "    return polars_df.to_pandas()\n",
    "\n",
    "def optimize_memory(df):\n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    return df\n",
    "\n",
    "class LoadData:\n",
    "    def __init__(self, file_paths, partition_ids=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.partition_ids = partition_ids\n",
    "\n",
    "    def load_and_concat(self):\n",
    "        if self.partition_ids is not None:\n",
    "            selected_files = [\n",
    "                fp for fp in self.file_paths\n",
    "                if any(f'partition_id={pid}' in fp for pid in self.partition_ids)\n",
    "            ]\n",
    "        else:\n",
    "            selected_files = self.file_paths\n",
    "\n",
    "        partitioned_data = [pl.read_parquet(file_path) for file_path in selected_files]\n",
    "        df = pl.concat(partitioned_data, rechunk=False)\n",
    "        \n",
    "        del partitioned_data\n",
    "        gc.collect()\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Example usage\n",
    "partition_ids = [6, 7, 8, 9]\n",
    "file_paths_all = sorted(glob.glob('Data/train.parquet/*/*.parquet'))\n",
    "loader = LoadData(file_paths=file_paths_all, partition_ids=partition_ids)\n",
    "df_selected = loader.load_and_concat()\n",
    "\n",
    "del loader, file_paths_all\n",
    "gc.collect()\n",
    "\n",
    "# Convert Polars to Pandas\n",
    "df_selected_pd = polars_to_pandas(df_selected)\n",
    "del df_selected\n",
    "gc.collect()\n",
    "\n",
    "# Sort by date_id ascending, then by time_id ascending\n",
    "df_selected_pd.sort_values(['date_id', 'time_id'], inplace=True)\n",
    "df_selected_pd.reset_index(drop=True, inplace=True)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_fill = [col for col in df_selected_pd.columns if col not in ['date_id', 'time_id', 'responder_6', 'weight']]\n",
    "\n",
    "window_size = 31 * 850\n",
    "\n",
    "for col in cols_to_fill:\n",
    "    # Rolling mean over 26350 rows +- 15 days + current day\n",
    "    rolling_mean_series = df_selected_pd[col].rolling(window=window_size, center=True, min_periods=1).mean()\n",
    "    df_selected_pd[col] = df_selected_pd[col].fillna(rolling_mean_series)\n",
    "\n",
    "df_selected_pd = df_selected_pd.fillna(0)\n",
    "optimize_memory(df_selected_pd)\n",
    "max_date_id = df_selected_pd['date_id'].max()\n",
    "split_index = max_date_id - 120\n",
    "df_train = df_selected_pd[df_selected_pd['date_id'] <= split_index]\n",
    "df_val = df_selected_pd[df_selected_pd['date_id'] > split_index]\n",
    "del df_selected_pd\n",
    "gc.collect()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[cols_to_fill])\n",
    "df_train[cols_to_fill] = scaler.transform(df_train[cols_to_fill])\n",
    "df_val[cols_to_fill] = scaler.transform(df_val[cols_to_fill])\n",
    "\n",
    "import joblib\n",
    "\n",
    "joblib.dump(scaler, 'scaler.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "####################################\n",
    "# 1) Dataset for On-The-Fly Sequences\n",
    "####################################\n",
    "\n",
    "class OnTheFlySequenceDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols, target_col, seq_length):\n",
    "        \"\"\"\n",
    "        df: DataFrame (already sorted, scaled, imputed) with columns including feature_cols, target_col, weight.\n",
    "        feature_cols: list of column names to use as features.\n",
    "        target_col: the name of the target column (e.g. 'responder_6').\n",
    "        seq_length: number of timesteps per sequence window.\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.feature_cols = feature_cols\n",
    "        self.target_col = target_col\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # The maximum valid index for starting a sequence of length seq_length\n",
    "        self.max_index = len(self.df) - self.seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_index\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_slice = self.df.loc[idx : idx + self.seq_length - 1, self.feature_cols]\n",
    "        y_slice = self.df.loc[idx + self.seq_length - 1, self.target_col]\n",
    "        w_slice = self.df.loc[idx + self.seq_length - 1, 'weight']\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        X = torch.tensor(x_slice.values, dtype=torch.float32)\n",
    "        y = torch.tensor(y_slice, dtype=torch.float32)\n",
    "        w = torch.tensor(w_slice, dtype=torch.float32)\n",
    "        return X, y, w\n",
    "\n",
    "####################################\n",
    "# 2) Weighted MSE & Weighted R²\n",
    "####################################\n",
    "\n",
    "def weighted_mse_loss(y_pred, y_true, w):\n",
    "    \"\"\"\n",
    "    Weighted MSE for training/backprop.\n",
    "    y_pred, y_true, w all shape: [batch_size].\n",
    "    \"\"\"\n",
    "    y_pred = y_pred.view(-1)\n",
    "    mse = (y_true - y_pred) ** 2\n",
    "    weighted_mse = w * mse\n",
    "    return torch.mean(weighted_mse)\n",
    "\n",
    "def compute_weighted_r2(model, loader, device):\n",
    "    \"\"\"\n",
    "    Compute Weighted R² over an entire dataset (loader).\n",
    "    R² = 1 - (Sum(w*(y - pred)^2) / Sum(w*y^2))\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, batch_w in loader:\n",
    "            batch_x, batch_y, batch_w = batch_x.to(device), batch_y.to(device), batch_w.to(device)\n",
    "            y_pred = model(batch_x).view(-1)\n",
    "            numerator += torch.sum(batch_w * (batch_y - y_pred) ** 2).item()\n",
    "            denominator += torch.sum(batch_w * (batch_y ** 2)).item()\n",
    "    if denominator == 0:\n",
    "        return 0.0\n",
    "    return 1.0 - (numerator / denominator)\n",
    "\n",
    "####################################\n",
    "# 3) Define a Two-Layer LSTM Model\n",
    "####################################\n",
    "\n",
    "class TwoLayerLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1, dropout=0.2):\n",
    "        super(TwoLayerLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,      # two LSTM layers\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_length, input_dim]\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        # Take the last time step\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)  # [batch_size, 1]\n",
    "        return out\n",
    "\n",
    "####################################\n",
    "# 4) Training Loop\n",
    "####################################\n",
    "\n",
    "def train_lstm(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader,\n",
    "    epochs=10,\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-5,\n",
    "    device='cpu'\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the LSTM using weighted MSE loss and track Weighted R² on validation.\n",
    "    Logs metrics to MLflow each epoch.\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for batch_x, batch_y, batch_w in train_loader:\n",
    "            batch_x, batch_y, batch_w = batch_x.to(device), batch_y.to(device), batch_w.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch_x).view(-1)  # shape: [batch_size]\n",
    "            loss = weighted_mse_loss(y_pred, batch_y, batch_w)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Optional: Gradient clipping for stability if sequences are large\n",
    "            # nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * len(batch_x)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Compute validation Weighted R²\n",
    "        val_weighted_r2 = compute_weighted_r2(model, val_loader, device)\n",
    "        \n",
    "        # Log metrics to MLflow\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_weighted_r2\", val_weighted_r2, step=epoch)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.6f}, \"\n",
    "              f\"Val Weighted R²: {val_weighted_r2:.6f}\")\n",
    "\n",
    "####################################\n",
    "# 5) Putting It All Together w/ MLflow\n",
    "####################################\n",
    "\n",
    "def main(df_train, df_val, cols_to_fill):\n",
    "    # Suppose df_train and df_val are your prepared DataFrames \n",
    "    # (already scaled, imputed, sorted, etc.)\n",
    "    \n",
    "    feature_cols = cols_to_fill  # your feature column names\n",
    "    target_col = 'responder_6'\n",
    "    seq_length = 100      # example\n",
    "    batch_size = 64\n",
    "    hidden_dim = 64\n",
    "    dropout = 0.2\n",
    "    lr = 1e-4\n",
    "    weight_decay = 1e-5\n",
    "    epochs = 10\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    # Create PyTorch Datasets\n",
    "    train_dataset = OnTheFlySequenceDataset(df_train, feature_cols, target_col, seq_length)\n",
    "    val_dataset   = OnTheFlySequenceDataset(df_val,   feature_cols, target_col, seq_length)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Define the model\n",
    "    model = TwoLayerLSTM(\n",
    "        input_dim=len(feature_cols), \n",
    "        hidden_dim=hidden_dim, \n",
    "        output_dim=1, \n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "    # Start MLflow run\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    mlflow.set_experiment(\"Financial_LSTM_Experiment\")\n",
    "    with mlflow.start_run(run_name=\"TwoLayerLSTM_CustomLoss\"):\n",
    "        \n",
    "        # Log hyperparameters to MLflow\n",
    "        mlflow.log_param(\"seq_length\", seq_length)\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        mlflow.log_param(\"hidden_dim\", hidden_dim)\n",
    "        mlflow.log_param(\"dropout\", dropout)\n",
    "        mlflow.log_param(\"learning_rate\", lr)\n",
    "        mlflow.log_param(\"weight_decay\", weight_decay)\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "\n",
    "        # Train\n",
    "        train_lstm(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            epochs=epochs,\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Final validation Weighted R²\n",
    "        final_val_r2 = compute_weighted_r2(model, val_loader, device)\n",
    "        mlflow.log_metric(\"final_val_weighted_r2\", final_val_r2)\n",
    "        \n",
    "        print(f\"Final Val Weighted R²: {final_val_r2:.6f}\")\n",
    "\n",
    "        # Log model to MLflow\n",
    "        mlflow.pytorch.log_model(model, artifact_path=\"lstm_model\")\n",
    "\n",
    "        # Cleanup\n",
    "        del df_train, df_val\n",
    "        gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(df_train, df_val, cols_to_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
