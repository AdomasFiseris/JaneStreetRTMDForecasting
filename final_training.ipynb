{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import joblib\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "\n",
    "def read_parquet_in_chunks(file_path, columns=None, chunk_size=1_000_000):\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    total_rows = parquet_file.metadata.num_rows\n",
    "    for start_row in range(0, total_rows, chunk_size):\n",
    "        end_row = min(start_row + chunk_size, total_rows)\n",
    "        batch = parquet_file.read_row_group(0, columns=columns, use_threads=True).slice(start_row, end_row - start_row)\n",
    "        yield batch.to_pandas()\n",
    "\n",
    "# Initialize max_date\n",
    "max_date = None\n",
    "file_paths = sorted(glob.glob('Data/train.parquet/*/*.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1/10: Data/train.parquet\\partition_id=0\\part-0.parquet\n",
      "Processing file 2/10: Data/train.parquet\\partition_id=1\\part-0.parquet\n",
      "Processing file 3/10: Data/train.parquet\\partition_id=2\\part-0.parquet\n",
      "Processing file 4/10: Data/train.parquet\\partition_id=3\\part-0.parquet\n",
      "Processing file 5/10: Data/train.parquet\\partition_id=4\\part-0.parquet\n",
      "Processing file 6/10: Data/train.parquet\\partition_id=5\\part-0.parquet\n",
      "Processing file 7/10: Data/train.parquet\\partition_id=6\\part-0.parquet\n",
      "Processing file 8/10: Data/train.parquet\\partition_id=7\\part-0.parquet\n",
      "Processing file 9/10: Data/train.parquet\\partition_id=8\\part-0.parquet\n",
      "Processing file 10/10: Data/train.parquet\\partition_id=9\\part-0.parquet\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LIBSVM file\n",
    "train_libsvm_file = 'final_train.libsvm'\n",
    "\n",
    "# Define the window size for rolling mean\n",
    "window_size = 800\n",
    "\n",
    "# Process each Parquet file\n",
    "for file_idx, file_path in enumerate(file_paths):\n",
    "    print(f\"Processing file {file_idx+1}/{len(file_paths)}: {file_path}\")\n",
    "    \n",
    "    # Read the file in chunks\n",
    "    for df_chunk in read_parquet_in_chunks(file_path, chunk_size=1_000_000):\n",
    "        \n",
    "        df_chunk = df_chunk.sort_values(['date_id', 'time_id'])\n",
    "        \n",
    "        # Ensure feature columns are float64 to avoid dtype issues\n",
    "        df_chunk = df_chunk.astype('float64')\n",
    "\n",
    "        # Exclude columns starting with 'responder_'\n",
    "        feature_cols = [col for col in df_chunk.columns if not col.startswith('responder_')]\n",
    "\n",
    "\n",
    "        feature_names_pickle = 'feature_columns.pkl'\n",
    "        pd.to_pickle(feature_cols, feature_names_pickle)\n",
    "        \n",
    "        # Process training data\n",
    "        if not df_chunk.empty:\n",
    "            X_train = df_chunk[feature_cols]\n",
    "            y_train = df_chunk['responder_6']\n",
    "\n",
    "            # Impute NaN values using backward-looking rolling mean\n",
    "            X_train_imputed = X_train.copy()\n",
    "            for col in feature_cols:\n",
    "                # Create a mask of NaN values\n",
    "                na_mask = X_train_imputed[col].isna()\n",
    "                if na_mask.any():\n",
    "                    # Forward fill to handle initial NaNs\n",
    "                    col_ffill = X_train_imputed[col].ffill()\n",
    "                    # Compute rolling mean (looking back)\n",
    "                    col_roll_mean = col_ffill.rolling(window=window_size, min_periods=1).mean()\n",
    "                    # Ensure data types are consistent\n",
    "                    col_roll_mean = col_roll_mean.astype(X_train_imputed[col].dtype)\n",
    "                    # Fill NaNs in the original series with rolling mean values\n",
    "                    X_train_imputed.loc[na_mask, col] = col_roll_mean[na_mask].astype(X_train_imputed[col].dtype)\n",
    "                    # If there are still NaNs (e.g., at the start), fill with overall mean\n",
    "                    if X_train_imputed[col].isna().any():\n",
    "                        overall_mean = X_train_imputed[col].mean()\n",
    "                        X_train_imputed[col] = X_train_imputed[col].fillna(overall_mean)\n",
    "            # Convert target variable to float64\n",
    "            y_train = y_train.astype('float64')\n",
    "\n",
    "            # Ensure no remaining NaNs\n",
    "            X_train_imputed = X_train_imputed.fillna(0)\n",
    "            \n",
    "            # Append to train LIBSVM file\n",
    "            with open(train_libsvm_file, 'ab') as f_train:\n",
    "                dump_svmlight_file(X_train_imputed, y_train, f_train, zero_based=True)\n",
    "\n",
    "        # Clean up\n",
    "        del df_chunk\n",
    "        gc.collect()\n",
    "\n",
    "        # Clean up training variables\n",
    "        if 'train_chunk' in locals():\n",
    "            del df_chunk\n",
    "        if 'X_train' in locals():\n",
    "            del X_train, y_train, X_train_imputed\n",
    "\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename(train_libsvm_file, 'final_train.libsvm.cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/24 21:47:57 INFO mlflow.tracking.fluent: Experiment with name 'XGBoost Full Dataset Training 2' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.97634\n",
      "[10]\ttrain-rmse:0.97542\n",
      "[20]\ttrain-rmse:0.97457\n",
      "[23]\ttrain-rmse:0.97436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afise\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [21:48:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\c_api\\c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b65ce528fd4c0c9c25f12b6a87eedb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/24 21:48:30 INFO mlflow.tracking._tracking_service.client: ðŸƒ View run Final Model Training at: http://localhost:5000/#/experiments/11/runs/3554add1244c466896fcac9a918a224c.\n",
      "2024/11/24 21:48:30 INFO mlflow.tracking._tracking_service.client: ðŸ§ª View experiment at: http://localhost:5000/#/experiments/11.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model training completed and logged.\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment('XGBoost Full Dataset Training 2')\n",
    "\n",
    "optimal_num_boost_round = 24\n",
    "\n",
    "params = {\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.0126,\n",
    "    'subsample': 0.6919,\n",
    "    'colsample_bytree': 0.6527,\n",
    "    'gamma': 0.3388,\n",
    "    'min_child_weight': 1,\n",
    "    'reg_alpha': 0.1218,\n",
    "    'reg_lambda': 2.7785,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': 'hist',\n",
    "    'eval_metric': 'rmse',\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "mlflow.xgboost.autolog(disable=True)\n",
    "\n",
    "with mlflow.start_run(run_name='Final Model Training'):\n",
    "\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_param('optimal_num_boost_round', optimal_num_boost_round)\n",
    "    \n",
    "    dfull = xgb.DMatrix('final_train.libsvm.cache?format=libsvm')\n",
    "    \n",
    "    # Since we don't have a validation set, we'll just monitor training metrics\n",
    "    evals = [(dfull, 'train')]\n",
    "    \n",
    "    # Train the model without early stopping\n",
    "    final_model = xgb.train(\n",
    "        params,\n",
    "        dfull,\n",
    "        num_boost_round=optimal_num_boost_round,\n",
    "        evals=evals,\n",
    "        verbose_eval=10\n",
    "    )\n",
    "    \n",
    "    # Save and log the final model using joblib\n",
    "    joblib.dump(final_model, 'final_model.joblib')\n",
    "    mlflow.log_artifact('final_model.joblib')\n",
    "    \n",
    "    # Log the model with MLflow\n",
    "    feature_names = pd.read_pickle('feature_columns.pkl')\n",
    "    input_example = pd.DataFrame([[0]*len(feature_names)], columns=feature_names, dtype='float64')\n",
    "    \n",
    "    mlflow.xgboost.log_model(final_model, artifact_path='model', input_example=input_example)\n",
    "    \n",
    "    # Log feature importance\n",
    "    importance = final_model.get_score(importance_type='gain')\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': list(importance.keys()),\n",
    "        'importance': list(importance.values())\n",
    "    })\n",
    "    importance_df.to_csv('feature_importance_full.csv', index=False)\n",
    "    mlflow.log_artifact('feature_importance_full.csv')\n",
    "    \n",
    "    print(\"Final model training completed and logged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.save_model('final_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
