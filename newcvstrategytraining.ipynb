{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique days in selected partitions: 339\n",
      "Validation will start from date_id: 1599\n",
      "Number of features: 82\n",
      "Features: ['date_id', 'time_id', 'symbol_id', 'feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', 'feature_05', 'feature_06', 'feature_07', 'feature_08', 'feature_09', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/02 21:30:33 INFO mlflow.tracking.fluent: Experiment with name 'JSF 8, 9 Last Two Partitions' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.82225\ttrain-weighted_r2:0.00029\tvalidation-rmse:0.79303\tvalidation-weighted_r2:0.00009\n",
      "[10]\ttrain-rmse:0.82119\ttrain-weighted_r2:0.00285\tvalidation-rmse:0.79272\tvalidation-weighted_r2:0.00086\n",
      "[20]\ttrain-rmse:0.82027\ttrain-weighted_r2:0.00510\tvalidation-rmse:0.79247\tvalidation-weighted_r2:0.00150\n",
      "[30]\ttrain-rmse:0.81945\ttrain-weighted_r2:0.00707\tvalidation-rmse:0.79225\tvalidation-weighted_r2:0.00206\n",
      "[40]\ttrain-rmse:0.81871\ttrain-weighted_r2:0.00886\tvalidation-rmse:0.79206\tvalidation-weighted_r2:0.00254\n",
      "[50]\ttrain-rmse:0.81803\ttrain-weighted_r2:0.01052\tvalidation-rmse:0.79189\tvalidation-weighted_r2:0.00295\n",
      "[60]\ttrain-rmse:0.81743\ttrain-weighted_r2:0.01196\tvalidation-rmse:0.79176\tvalidation-weighted_r2:0.00328\n",
      "[70]\ttrain-rmse:0.81689\ttrain-weighted_r2:0.01327\tvalidation-rmse:0.79164\tvalidation-weighted_r2:0.00360\n",
      "[80]\ttrain-rmse:0.81639\ttrain-weighted_r2:0.01449\tvalidation-rmse:0.79153\tvalidation-weighted_r2:0.00386\n",
      "[90]\ttrain-rmse:0.81593\ttrain-weighted_r2:0.01559\tvalidation-rmse:0.79142\tvalidation-weighted_r2:0.00414\n",
      "[100]\ttrain-rmse:0.81550\ttrain-weighted_r2:0.01663\tvalidation-rmse:0.79134\tvalidation-weighted_r2:0.00434\n",
      "[110]\ttrain-rmse:0.81507\ttrain-weighted_r2:0.01766\tvalidation-rmse:0.79127\tvalidation-weighted_r2:0.00452\n",
      "[120]\ttrain-rmse:0.81469\ttrain-weighted_r2:0.01859\tvalidation-rmse:0.79121\tvalidation-weighted_r2:0.00467\n",
      "[130]\ttrain-rmse:0.81433\ttrain-weighted_r2:0.01944\tvalidation-rmse:0.79115\tvalidation-weighted_r2:0.00483\n",
      "[140]\ttrain-rmse:0.81399\ttrain-weighted_r2:0.02026\tvalidation-rmse:0.79110\tvalidation-weighted_r2:0.00495\n",
      "[150]\ttrain-rmse:0.81369\ttrain-weighted_r2:0.02098\tvalidation-rmse:0.79104\tvalidation-weighted_r2:0.00511\n",
      "[160]\ttrain-rmse:0.81333\ttrain-weighted_r2:0.02187\tvalidation-rmse:0.79101\tvalidation-weighted_r2:0.00517\n",
      "[170]\ttrain-rmse:0.81302\ttrain-weighted_r2:0.02260\tvalidation-rmse:0.79098\tvalidation-weighted_r2:0.00525\n",
      "[180]\ttrain-rmse:0.81274\ttrain-weighted_r2:0.02327\tvalidation-rmse:0.79094\tvalidation-weighted_r2:0.00534\n",
      "[190]\ttrain-rmse:0.81246\ttrain-weighted_r2:0.02395\tvalidation-rmse:0.79092\tvalidation-weighted_r2:0.00541\n",
      "[200]\ttrain-rmse:0.81218\ttrain-weighted_r2:0.02463\tvalidation-rmse:0.79090\tvalidation-weighted_r2:0.00546\n",
      "[210]\ttrain-rmse:0.81190\ttrain-weighted_r2:0.02529\tvalidation-rmse:0.79089\tvalidation-weighted_r2:0.00548\n",
      "[220]\ttrain-rmse:0.81161\ttrain-weighted_r2:0.02598\tvalidation-rmse:0.79087\tvalidation-weighted_r2:0.00553\n",
      "[230]\ttrain-rmse:0.81128\ttrain-weighted_r2:0.02677\tvalidation-rmse:0.79084\tvalidation-weighted_r2:0.00561\n",
      "[240]\ttrain-rmse:0.81106\ttrain-weighted_r2:0.02731\tvalidation-rmse:0.79081\tvalidation-weighted_r2:0.00567\n",
      "[250]\ttrain-rmse:0.81075\ttrain-weighted_r2:0.02804\tvalidation-rmse:0.79079\tvalidation-weighted_r2:0.00573\n",
      "[260]\ttrain-rmse:0.81047\ttrain-weighted_r2:0.02872\tvalidation-rmse:0.79076\tvalidation-weighted_r2:0.00580\n",
      "[270]\ttrain-rmse:0.81022\ttrain-weighted_r2:0.02932\tvalidation-rmse:0.79075\tvalidation-weighted_r2:0.00582\n",
      "[280]\ttrain-rmse:0.80995\ttrain-weighted_r2:0.02997\tvalidation-rmse:0.79073\tvalidation-weighted_r2:0.00588\n",
      "[290]\ttrain-rmse:0.80972\ttrain-weighted_r2:0.03052\tvalidation-rmse:0.79072\tvalidation-weighted_r2:0.00590\n",
      "[300]\ttrain-rmse:0.80949\ttrain-weighted_r2:0.03108\tvalidation-rmse:0.79071\tvalidation-weighted_r2:0.00592\n",
      "[310]\ttrain-rmse:0.80922\ttrain-weighted_r2:0.03173\tvalidation-rmse:0.79071\tvalidation-weighted_r2:0.00593\n",
      "[320]\ttrain-rmse:0.80895\ttrain-weighted_r2:0.03237\tvalidation-rmse:0.79069\tvalidation-weighted_r2:0.00598\n",
      "[330]\ttrain-rmse:0.80873\ttrain-weighted_r2:0.03288\tvalidation-rmse:0.79067\tvalidation-weighted_r2:0.00602\n",
      "[340]\ttrain-rmse:0.80851\ttrain-weighted_r2:0.03343\tvalidation-rmse:0.79067\tvalidation-weighted_r2:0.00604\n",
      "[350]\ttrain-rmse:0.80823\ttrain-weighted_r2:0.03409\tvalidation-rmse:0.79066\tvalidation-weighted_r2:0.00606\n",
      "[360]\ttrain-rmse:0.80798\ttrain-weighted_r2:0.03468\tvalidation-rmse:0.79065\tvalidation-weighted_r2:0.00608\n",
      "[370]\ttrain-rmse:0.80772\ttrain-weighted_r2:0.03530\tvalidation-rmse:0.79063\tvalidation-weighted_r2:0.00613\n",
      "[380]\ttrain-rmse:0.80751\ttrain-weighted_r2:0.03579\tvalidation-rmse:0.79062\tvalidation-weighted_r2:0.00615\n",
      "[390]\ttrain-rmse:0.80716\ttrain-weighted_r2:0.03664\tvalidation-rmse:0.79061\tvalidation-weighted_r2:0.00617\n",
      "[399]\ttrain-rmse:0.80694\ttrain-weighted_r2:0.03717\tvalidation-rmse:0.79061\tvalidation-weighted_r2:0.00617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/02 21:38:04 INFO mlflow.tracking._tracking_service.client: 🏃 View run Final Training with all Partitions at: http://localhost:5000/#/experiments/37/runs/88b557e7ee2344bfbd85b91a528960e3.\n",
      "2024/12/02 21:38:04 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/37.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import mlflow\n",
    "import joblib\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def polars_to_pandas(polars_df):\n",
    "    \"\"\"Convert a Polars DataFrame to a Pandas DataFrame.\"\"\"\n",
    "    return polars_df.to_pandas()\n",
    "\n",
    "def optimize_memory(df):\n",
    "    \"\"\"Downcast numerical columns to reduce memory usage.\"\"\"\n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    return df\n",
    "\n",
    "def plot_feature_importance(importance_df, title='Feature Importance'):\n",
    "    \"\"\"Plot and save feature importance.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance (Gain)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plot_filename = f\"{title.replace(' ', '_').lower()}.png\"\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    return plot_filename\n",
    "\n",
    "class LoadData:\n",
    "    def __init__(self, file_paths, partition_ids=None):\n",
    "        \"\"\"\n",
    "        Initialize the LoadData class.\n",
    "\n",
    "        :param file_paths: List of all file paths.\n",
    "        :param partition_ids: List of partition IDs to load. If None, load all.\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.partition_ids = partition_ids\n",
    "\n",
    "    def load_and_concat(self):\n",
    "        \"\"\"Load and concatenate Polars DataFrames from specified file paths.\"\"\"\n",
    "        if self.partition_ids is not None:\n",
    "            # Filter file paths to include only specified partitions\n",
    "            selected_files = [\n",
    "                fp for fp in self.file_paths\n",
    "                if any(f'partition_id={pid}' in fp for pid in self.partition_ids)\n",
    "            ]\n",
    "        else:\n",
    "            selected_files = self.file_paths\n",
    "\n",
    "        # Load each Parquet file into a Polars DataFrame\n",
    "        partitioned_data = [pl.read_parquet(file_path) for file_path in selected_files]\n",
    "        \n",
    "        # Concatenate all DataFrames into one\n",
    "        df = pl.concat(partitioned_data, rechunk=False)\n",
    "        \n",
    "        # Delete individual DataFrames to free memory\n",
    "        del partitioned_data\n",
    "        gc.collect()\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Specify partition IDs to load (8, 9)\n",
    "partition_ids = [8, 9]\n",
    "\n",
    "# Get all file paths sorted (adjust the glob pattern as needed)\n",
    "file_paths_all = sorted(glob.glob('Data/train.parquet/*/*.parquet'))\n",
    "\n",
    "# Initialize the loader with selected partitions\n",
    "loader = LoadData(file_paths=file_paths_all, partition_ids=partition_ids)\n",
    "\n",
    "# Load and concatenate the selected partitions\n",
    "df_selected = loader.load_and_concat()\n",
    "\n",
    "# Free up memory by deleting the loader and file paths\n",
    "del loader, file_paths_all\n",
    "gc.collect()\n",
    "\n",
    "# Convert to Pandas for easier manipulation\n",
    "df_selected_pd = polars_to_pandas(df_selected)\n",
    "\n",
    "# Free up memory by deleting the Polars DataFrame\n",
    "del df_selected\n",
    "gc.collect()\n",
    "\n",
    "# Sort by 'date_id' ascending, then by 'time_id' ascending\n",
    "df_selected_pd.sort_values(['date_id', 'time_id'], inplace=True)\n",
    "df_selected_pd.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Invoke garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Identify unique date_ids sorted in ascending order\n",
    "unique_date_ids_sorted = np.sort(df_selected_pd['date_id'].unique())\n",
    "\n",
    "# Total number of unique days\n",
    "total_unique_days = len(unique_date_ids_sorted)\n",
    "print(f\"Total unique days in selected partitions: {total_unique_days}\")\n",
    "\n",
    "# Define the number of validation days\n",
    "validation_days = 100\n",
    "\n",
    "# Determine the cutoff index\n",
    "cutoff_index = total_unique_days - validation_days\n",
    "\n",
    "# Get the cutoff date_id\n",
    "validation_start_date_id = unique_date_ids_sorted[cutoff_index]\n",
    "print(f\"Validation will start from date_id: {validation_start_date_id}\")\n",
    "\n",
    "# Split into training and validation sets based on date_id\n",
    "train_df = df_selected_pd[df_selected_pd['date_id'] < validation_start_date_id].copy()\n",
    "validation_df = df_selected_pd[df_selected_pd['date_id'] >= validation_start_date_id].copy()\n",
    "\n",
    "# Free up memory by deleting the sorted DataFrame\n",
    "del df_selected_pd\n",
    "gc.collect()\n",
    "\n",
    "# Define non-informative features (modify based on your dataset)\n",
    "excluded_features = [col for col in train_df.columns if col.startswith('responder_')] + ['weight']\n",
    "\n",
    "# Define feature columns\n",
    "feature_cols = [col for col in train_df.columns if col not in excluded_features]\n",
    "\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"Features: {feature_cols}\")\n",
    "\n",
    "# Optimize memory by downcasting\n",
    "train_df = optimize_memory(train_df)\n",
    "validation_df = optimize_memory(validation_df)\n",
    "\n",
    "# Training set\n",
    "X_train = train_df[feature_cols].astype(np.float32)\n",
    "y_train = train_df['responder_6'].astype(np.float32)\n",
    "w_train = train_df['weight'].astype(np.float32)\n",
    "\n",
    "# Validation set\n",
    "X_val = validation_df[feature_cols].astype(np.float32)\n",
    "y_val = validation_df['responder_6'].astype(np.float32)\n",
    "w_val = validation_df['weight'].astype(np.float32)\n",
    "\n",
    "# Free up memory by deleting the DataFrames\n",
    "del train_df, validation_df\n",
    "gc.collect()\n",
    "\n",
    "# Create DMatrix for training and validation\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train, weight=w_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val, weight=w_val)\n",
    "\n",
    "# Free up memory by deleting the Pandas DataFrames\n",
    "gc.collect()\n",
    "\n",
    "def weighted_r2_metric(preds, dtrain):\n",
    "    \"\"\"Custom weighted R² evaluation metric for XGBoost.\"\"\"\n",
    "    y_true = dtrain.get_label()\n",
    "    w = dtrain.get_weight()\n",
    "    numerator = np.sum(w * (y_true - preds) ** 2)\n",
    "    denominator = np.sum(w * y_true ** 2)\n",
    "    r2 = 1 - (numerator / denominator)\n",
    "    return 'weighted_r2', r2\n",
    "\n",
    "# Best hyperparameters from your grid search (16th combination)\n",
    "best_params = {\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.0126,\n",
    "    'subsample': 0.6919,\n",
    "    'colsample_bytree': 0.6527,\n",
    "    'gamma': 0.3388,\n",
    "    'min_child_weight': 1,\n",
    "    'reg_alpha': 0.1218,\n",
    "    'reg_lambda': 2.7785,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': 'hist',\n",
    "    'eval_metric': 'rmse',\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Set up MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")  # Update if different\n",
    "mlflow.set_experiment('JSF 8, 9 Last Two Partitions')\n",
    "\n",
    "# Disable automatic MLflow logging to customize\n",
    "mlflow.xgboost.autolog(disable=True)\n",
    "\n",
    "with mlflow.start_run(run_name='Final Training with all Partitions'):\n",
    "\n",
    "    # Log hyperparameters\n",
    "    mlflow.log_params(best_params)\n",
    "    \n",
    "    # Define evaluation sets\n",
    "    evals = [(dtrain, 'train'), (dval, 'validation')]\n",
    "    \n",
    "    # Train the model\n",
    "    model = xgb.train(\n",
    "        params=best_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=10,\n",
    "        custom_metric=weighted_r2_metric,\n",
    "        maximize=True,\n",
    "        verbose_eval=10\n",
    "    )\n",
    "    \n",
    "    # Log the best iteration\n",
    "    mlflow.log_metric('best_iteration', model.best_iteration)\n",
    "    \n",
    "    # Log custom metrics\n",
    "    y_pred_val = model.predict(dval)\n",
    "    weighted_r2_val = 1 - (np.sum(w_val * (y_val - y_pred_val) ** 2) / np.sum(w_val * y_val ** 2))\n",
    "    mlflow.log_metric('validation_weighted_r2', weighted_r2_val)\n",
    "    \n",
    "    y_pred_train = model.predict(dtrain)\n",
    "    weighted_r2_train = 1 - (np.sum(w_train * (y_train - y_pred_train) ** 2) / np.sum(w_train * y_train ** 2))\n",
    "    mlflow.log_metric('training_weighted_r2', weighted_r2_train)\n",
    "    \n",
    "    rmse_val = np.sqrt(mean_squared_error(y_val, y_pred_val, sample_weight=w_val))\n",
    "    mlflow.log_metric('validation_rmse', rmse_val)\n",
    "    \n",
    "    # Optionally save and log the model\n",
    "    joblib.dump(model, 'final_model.joblib')\n",
    "    mlflow.log_artifact('final_model.joblib')\n",
    "    \n",
    "    # Clean up\n",
    "    del dtrain, dval, model, y_pred_val, y_pred_train, weighted_r2_val, weighted_r2_train, rmse_val\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Final Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import mlflow\n",
    "import joblib\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def polars_to_pandas(polars_df):\n",
    "    \"\"\"Convert a Polars DataFrame to a Pandas DataFrame.\"\"\"\n",
    "    return polars_df.to_pandas()\n",
    "\n",
    "def optimize_memory(df):\n",
    "    \"\"\"Downcast numerical columns to reduce memory usage.\"\"\"\n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    return df\n",
    "\n",
    "def plot_feature_importance(importance_df, title='Feature Importance'):\n",
    "    \"\"\"Plot and save feature importance.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance (Gain)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plot_filename = f\"{title.replace(' ', '_').lower()}.png\"\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    return plot_filename\n",
    "\n",
    "class LoadData:\n",
    "    def __init__(self, file_paths, partition_ids=None):\n",
    "        \"\"\"\n",
    "        Initialize the LoadData class.\n",
    "\n",
    "        :param file_paths: List of all file paths.\n",
    "        :param partition_ids: List of partition IDs to load. If None, load all.\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.partition_ids = partition_ids\n",
    "\n",
    "    def load_and_concat(self):\n",
    "        \"\"\"Load and concatenate Polars DataFrames from specified file paths.\"\"\"\n",
    "        if self.partition_ids is not None:\n",
    "            # Filter file paths to include only specified partitions\n",
    "            selected_files = [\n",
    "                fp for fp in self.file_paths\n",
    "                if any(f'partition_id={pid}' in fp for pid in self.partition_ids)\n",
    "            ]\n",
    "        else:\n",
    "            selected_files = self.file_paths\n",
    "\n",
    "        # Load each Parquet file into a Polars DataFrame\n",
    "        partitioned_data = [pl.read_parquet(file_path) for file_path in selected_files]\n",
    "        \n",
    "        # Concatenate all DataFrames into one\n",
    "        df = pl.concat(partitioned_data, rechunk=False)\n",
    "        \n",
    "        # Delete individual DataFrames to free memory\n",
    "        del partitioned_data\n",
    "        gc.collect()\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Specify partition IDs to load (5, 6, 7, 8, 9)\n",
    "partition_ids = [5, 6, 7, 8, 9]\n",
    "\n",
    "# Get all file paths sorted (adjust the glob pattern as needed)\n",
    "file_paths_all = sorted(glob.glob('Data/train.parquet/*/*.parquet'))\n",
    "\n",
    "# Initialize the loader with selected partitions\n",
    "loader = LoadData(file_paths=file_paths_all, partition_ids=partition_ids)\n",
    "\n",
    "# Load and concatenate the selected partitions\n",
    "df_selected = loader.load_and_concat()\n",
    "\n",
    "# Free up memory by deleting the loader and file paths\n",
    "del loader, file_paths_all\n",
    "gc.collect()\n",
    "\n",
    "# Convert to Pandas for easier manipulation\n",
    "df_selected_pd = polars_to_pandas(df_selected)\n",
    "\n",
    "# Free up memory by deleting the Polars DataFrame\n",
    "del df_selected\n",
    "gc.collect()\n",
    "\n",
    "# Sort by 'date_id' ascending, then by 'time_id' ascending\n",
    "df_selected_pd.sort_values(['date_id', 'time_id'], inplace=True)\n",
    "df_selected_pd.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Invoke garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Split into training and validation sets based on date_id\n",
    "train_df = df_selected_pd.copy()\n",
    "\n",
    "# Free up memory by deleting the sorted DataFrame\n",
    "del df_selected_pd\n",
    "gc.collect()\n",
    "\n",
    "# Define non-informative features (modify based on your dataset)\n",
    "excluded_features = [col for col in train_df.columns if col.startswith('responder_')] + ['weight']\n",
    "\n",
    "# Define feature columns\n",
    "feature_cols = [col for col in train_df.columns if col not in excluded_features]\n",
    "\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"Features: {feature_cols}\")\n",
    "\n",
    "# Optimize memory by downcasting\n",
    "train_df = optimize_memory(train_df)\n",
    "\n",
    "# Training set\n",
    "X_train = train_df[feature_cols].astype(np.float32)\n",
    "y_train = train_df['responder_6'].astype(np.float32)\n",
    "w_train = train_df['weight'].astype(np.float32)\n",
    "\n",
    "# Free up memory by deleting the DataFrames\n",
    "del train_df\n",
    "gc.collect()\n",
    "\n",
    "# Create DMatrix for training and validation\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train, weight=w_train)\n",
    "\n",
    "# Free up memory by deleting the Pandas DataFrames\n",
    "gc.collect()\n",
    "\n",
    "def weighted_r2_metric(preds, dtrain):\n",
    "    \"\"\"Custom weighted R² evaluation metric for XGBoost.\"\"\"\n",
    "    y_true = dtrain.get_label()\n",
    "    w = dtrain.get_weight()\n",
    "    numerator = np.sum(w * (y_true - preds) ** 2)\n",
    "    denominator = np.sum(w * y_true ** 2)\n",
    "    r2 = 1 - (numerator / denominator)\n",
    "    return 'weighted_r2', r2\n",
    "\n",
    "# Best hyperparameters from your grid search (16th combination)\n",
    "best_params = {\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.0126,\n",
    "    'subsample': 0.6919,\n",
    "    'colsample_bytree': 0.6527,\n",
    "    'gamma': 0.3388,\n",
    "    'min_child_weight': 1,\n",
    "    'reg_alpha': 0.1218,\n",
    "    'reg_lambda': 2.7785,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': 'hist',\n",
    "    'eval_metric': 'rmse',\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Set up MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")  # Update if different\n",
    "mlflow.set_experiment('JSF 5, 6, 7, 8, 9 Old HP Final')\n",
    "\n",
    "# Disable automatic MLflow logging to customize\n",
    "mlflow.xgboost.autolog(disable=True)\n",
    "\n",
    "with mlflow.start_run(run_name='Final Training with all Partitions'):\n",
    "\n",
    "    # Log hyperparameters\n",
    "    mlflow.log_params(best_params)\n",
    "    \n",
    "    # Define evaluation sets\n",
    "    evals = [(dtrain, 'train')]\n",
    "    \n",
    "    # Train the model\n",
    "    model = xgb.train(\n",
    "        params=best_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=447,\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=10,\n",
    "        custom_metric=weighted_r2_metric,\n",
    "        maximize=True,\n",
    "        verbose_eval=10\n",
    "    )\n",
    "    \n",
    "    # Log the best iteration\n",
    "    mlflow.log_metric('best_iteration', model.best_iteration)\n",
    "    \n",
    "    y_pred_train = model.predict(dtrain)\n",
    "    weighted_r2_train = 1 - (np.sum(w_train * (y_train - y_pred_train) ** 2) / np.sum(w_train * y_train ** 2))\n",
    "    mlflow.log_metric('training_weighted_r2', weighted_r2_train)\n",
    "    \n",
    "    # Optionally save and log the model\n",
    "    joblib.dump(model, 'final_model.joblib')\n",
    "    mlflow.log_artifact('final_model.joblib')\n",
    "    \n",
    "    # Clean up\n",
    "    del dtrain, y_pred_train, weighted_r2_train\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model('oldhp_0071_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
