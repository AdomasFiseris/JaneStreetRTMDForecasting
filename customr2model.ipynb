{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import mlflow\n",
    "import joblib\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def polars_to_pandas(polars_df):\n",
    "    \"\"\"Convert a Polars DataFrame to a Pandas DataFrame.\"\"\"\n",
    "    return polars_df.to_pandas()\n",
    "\n",
    "def optimize_memory(df):\n",
    "    \"\"\"Downcast numerical columns to reduce memory usage.\"\"\"\n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    return df\n",
    "\n",
    "def plot_feature_importance(importance_df, title='Feature Importance'):\n",
    "    \"\"\"Plot and save feature importance.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance (Gain)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plot_filename = f\"{title.replace(' ', '_').lower()}.png\"\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    return plot_filename\n",
    "\n",
    "class LoadData:\n",
    "    def __init__(self, file_paths, partition_ids=None):\n",
    "        \"\"\"\n",
    "        Initialize the LoadData class.\n",
    "\n",
    "        :param file_paths: List of all file paths.\n",
    "        :param partition_ids: List of partition IDs to load. If None, load all.\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.partition_ids = partition_ids\n",
    "\n",
    "    def load_and_concat(self):\n",
    "        \"\"\"Load and concatenate Polars DataFrames from specified file paths.\"\"\"\n",
    "        if self.partition_ids is not None:\n",
    "            # Filter file paths to include only specified partitions\n",
    "            selected_files = [\n",
    "                fp for fp in self.file_paths\n",
    "                if any(f'partition_id={pid}' in fp for pid in self.partition_ids)\n",
    "            ]\n",
    "        else:\n",
    "            selected_files = self.file_paths\n",
    "\n",
    "        # Load each Parquet file into a Polars DataFrame\n",
    "        partitioned_data = [pl.read_parquet(file_path) for file_path in selected_files]\n",
    "        \n",
    "        # Concatenate all DataFrames into one\n",
    "        df = pl.concat(partitioned_data, rechunk=False)\n",
    "        \n",
    "        # Delete individual DataFrames to free memory\n",
    "        del partitioned_data\n",
    "        gc.collect()\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Specify partition IDs to load (6, 7, 8, 9)\n",
    "partition_ids = [6, 7, 8, 9]\n",
    "\n",
    "# Get all file paths sorted (adjust the glob pattern as needed)\n",
    "file_paths_all = sorted(glob.glob('Data/train.parquet/*/*.parquet'))\n",
    "\n",
    "# Initialize the loader with selected partitions\n",
    "loader = LoadData(file_paths=file_paths_all, partition_ids=partition_ids)\n",
    "\n",
    "# Load and concatenate the selected partitions\n",
    "df_selected = loader.load_and_concat()\n",
    "\n",
    "# Free up memory by deleting the loader and file paths\n",
    "del loader, file_paths_all\n",
    "gc.collect()\n",
    "\n",
    "# Convert to Pandas for easier manipulation\n",
    "df_selected_pd = polars_to_pandas(df_selected)\n",
    "\n",
    "# Free up memory by deleting the Polars DataFrame\n",
    "del df_selected\n",
    "gc.collect()\n",
    "\n",
    "# Sort by 'date_id' ascending, then by 'time_id' ascending\n",
    "df_selected_pd.sort_values(['date_id', 'time_id'], inplace=True)\n",
    "df_selected_pd.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Invoke garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Identify unique date_ids sorted in ascending order\n",
    "unique_date_ids_sorted = np.sort(df_selected_pd['date_id'].unique())\n",
    "\n",
    "# Total number of unique days\n",
    "total_unique_days = len(unique_date_ids_sorted)\n",
    "print(f\"Total unique days in selected partitions: {total_unique_days}\")\n",
    "\n",
    "# Define the number of validation days\n",
    "validation_days = 100\n",
    "\n",
    "# Determine the cutoff index\n",
    "cutoff_index = total_unique_days - validation_days\n",
    "\n",
    "# Get the cutoff date_id\n",
    "validation_start_date_id = unique_date_ids_sorted[cutoff_index]\n",
    "print(f\"Validation will start from date_id: {validation_start_date_id}\")\n",
    "\n",
    "# Split into training and validation sets based on date_id\n",
    "train_df = df_selected_pd[df_selected_pd['date_id'] < validation_start_date_id].copy()\n",
    "validation_df = df_selected_pd[df_selected_pd['date_id'] >= validation_start_date_id].copy()\n",
    "\n",
    "# Free up memory by deleting the sorted DataFrame\n",
    "del df_selected_pd\n",
    "gc.collect()\n",
    "\n",
    "# Define non-informative features (modify based on your dataset)\n",
    "excluded_features = [col for col in train_df.columns if col.startswith('responder_')] + ['weight']\n",
    "\n",
    "# Define feature columns\n",
    "feature_cols = [col for col in train_df.columns if col not in excluded_features]\n",
    "\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"Features: {feature_cols}\")\n",
    "\n",
    "# Optimize memory by downcasting\n",
    "train_df = optimize_memory(train_df)\n",
    "validation_df = optimize_memory(validation_df)\n",
    "\n",
    "# Training set\n",
    "X_train = train_df[feature_cols].astype(np.float32)\n",
    "y_train = train_df['responder_6'].astype(np.float32)\n",
    "w_train = train_df['weight'].astype(np.float32)\n",
    "\n",
    "# Validation set\n",
    "X_val = validation_df[feature_cols].astype(np.float32)\n",
    "y_val = validation_df['responder_6'].astype(np.float32)\n",
    "w_val = validation_df['weight'].astype(np.float32)\n",
    "\n",
    "# Free up memory by deleting the DataFrames\n",
    "del train_df, validation_df\n",
    "gc.collect()\n",
    "\n",
    "# Create DMatrix for training and validation\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train, weight=w_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val, weight=w_val)\n",
    "\n",
    "# Free up memory by deleting the Pandas DataFrames\n",
    "gc.collect()\n",
    "\n",
    "def weighted_r2_metric(preds, dtrain):\n",
    "    \"\"\"Custom weighted RÂ² evaluation metric for XGBoost.\"\"\"\n",
    "    y_true = dtrain.get_label()\n",
    "    w = dtrain.get_weight()\n",
    "    numerator = np.sum(w * (y_true - preds) ** 2)\n",
    "    denominator = np.sum(w * y_true ** 2)\n",
    "    r2 = 1 - (numerator / denominator)\n",
    "    return 'weighted_r2', r2\n",
    "\n",
    "# Best hyperparameters from your grid search\n",
    "best_params = {\n",
    "    'max_depth': 4,\n",
    "    'learning_rate': 0.09996,\n",
    "    'subsample': 0.70679,\n",
    "    'colsample_bytree': 0.73263,\n",
    "    'gamma': 0.26034,\n",
    "    'min_child_weight': 3,\n",
    "    'reg_alpha': 0.43891,\n",
    "    'reg_lambda': 0.59725,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': 'hist',\n",
    "    'eval_metric': 'rmse',  # Base evaluation metric\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 1,\n",
    "    'seed': 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")  # Update if different\n",
    "mlflow.set_experiment('Jane Street Forecasting Custom R2 Validation 6')\n",
    "\n",
    "# Disable automatic MLflow logging to customize\n",
    "mlflow.xgboost.autolog(disable=True)\n",
    "\n",
    "with mlflow.start_run(run_name='Final Training with Last 4 Partitions'):\n",
    "\n",
    "    # Log hyperparameters\n",
    "    mlflow.log_params(best_params)\n",
    "    \n",
    "    # Define evaluation sets\n",
    "    evals = [(dtrain, 'train'), (dval, 'validation')]\n",
    "    \n",
    "    # Train the model\n",
    "    model = xgb.train(\n",
    "        params=best_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=10,\n",
    "        custom_metric=weighted_r2_metric,\n",
    "        maximize=True,\n",
    "        verbose_eval=10\n",
    "    )\n",
    "    \n",
    "    # Log the best iteration\n",
    "    mlflow.log_metric('best_iteration', model.best_iteration)\n",
    "    \n",
    "    # Log custom metrics\n",
    "    y_pred_val = model.predict(dval)\n",
    "    weighted_r2_val = 1 - (np.sum(w_val * (y_val - y_pred_val) ** 2) / np.sum(w_val * y_val ** 2))\n",
    "    mlflow.log_metric('validation_weighted_r2', weighted_r2_val)\n",
    "    \n",
    "    y_pred_train = model.predict(dtrain)\n",
    "    weighted_r2_train = 1 - (np.sum(w_train * (y_train - y_pred_train) ** 2) / np.sum(w_train * y_train ** 2))\n",
    "    mlflow.log_metric('training_weighted_r2', weighted_r2_train)\n",
    "    \n",
    "    rmse_val = np.sqrt(mean_squared_error(y_val, y_pred_val, sample_weight=w_val))\n",
    "    mlflow.log_metric('validation_rmse', rmse_val)\n",
    "    \n",
    "    # Optionally save and log the model\n",
    "    joblib.dump(model, 'final_model.joblib')\n",
    "    mlflow.log_artifact('final_model.joblib')\n",
    "    \n",
    "    # Clean up\n",
    "    del dtrain, dval, model, y_pred_val, y_pred_train, weighted_r2_val, weighted_r2_train, rmse_val\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Final Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import mlflow\n",
    "import joblib\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def polars_to_pandas(polars_df):\n",
    "    \"\"\"Convert a Polars DataFrame to a Pandas DataFrame.\"\"\"\n",
    "    return polars_df.to_pandas()\n",
    "\n",
    "def optimize_memory(df):\n",
    "    \"\"\"Downcast numerical columns to reduce memory usage.\"\"\"\n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    return df\n",
    "\n",
    "def plot_feature_importance(importance_df, title='Feature Importance'):\n",
    "    \"\"\"Plot and save feature importance.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance (Gain)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plot_filename = f\"{title.replace(' ', '_').lower()}.png\"\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    return plot_filename\n",
    "\n",
    "class LoadData:\n",
    "    def __init__(self, file_paths, partition_ids=None):\n",
    "        \"\"\"\n",
    "        Initialize the LoadData class.\n",
    "\n",
    "        :param file_paths: List of all file paths.\n",
    "        :param partition_ids: List of partition IDs to load. If None, load all.\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.partition_ids = partition_ids\n",
    "\n",
    "    def load_and_concat(self):\n",
    "        \"\"\"Load and concatenate Polars DataFrames from specified file paths.\"\"\"\n",
    "        if self.partition_ids is not None:\n",
    "            # Filter file paths to include only specified partitions\n",
    "            selected_files = [\n",
    "                fp for fp in self.file_paths\n",
    "                if any(f'partition_id={pid}' in fp for pid in self.partition_ids)\n",
    "            ]\n",
    "        else:\n",
    "            selected_files = self.file_paths\n",
    "\n",
    "        # Load each Parquet file into a Polars DataFrame\n",
    "        partitioned_data = [pl.read_parquet(file_path) for file_path in selected_files]\n",
    "        \n",
    "        # Concatenate all DataFrames into one\n",
    "        df = pl.concat(partitioned_data, rechunk=False)\n",
    "        \n",
    "        # Delete individual DataFrames to free memory\n",
    "        del partitioned_data\n",
    "        gc.collect()\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Specify partition IDs to load (6, 7, 8, 9)\n",
    "partition_ids = [6, 7, 8, 9]\n",
    "\n",
    "# Get all file paths sorted (adjust the glob pattern as needed)\n",
    "file_paths_all = sorted(glob.glob('Data/train.parquet/*/*.parquet'))\n",
    "\n",
    "# Initialize the loader with selected partitions\n",
    "loader = LoadData(file_paths=file_paths_all, partition_ids=partition_ids)\n",
    "\n",
    "# Load and concatenate the selected partitions\n",
    "df_selected = loader.load_and_concat()\n",
    "\n",
    "# Free up memory by deleting the loader and file paths\n",
    "del loader, file_paths_all\n",
    "gc.collect()\n",
    "\n",
    "# Convert to Pandas for easier manipulation\n",
    "df_selected_pd = polars_to_pandas(df_selected)\n",
    "\n",
    "# Free up memory by deleting the Polars DataFrame\n",
    "del df_selected\n",
    "gc.collect()\n",
    "\n",
    "# Sort by 'date_id' ascending, then by 'time_id' ascending\n",
    "df_selected_pd.sort_values(['date_id', 'time_id'], inplace=True)\n",
    "df_selected_pd.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Invoke garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Define non-informative features (modify based on your dataset)\n",
    "excluded_features = [col for col in df_selected_pd.columns if col.startswith('responder_')] + ['weight']\n",
    "\n",
    "# Define feature columns\n",
    "feature_cols = [col for col in df_selected_pd.columns if col not in excluded_features]\n",
    "\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"Features: {feature_cols}\")\n",
    "\n",
    "# Optimize memory by downcasting\n",
    "df_selected_pd = optimize_memory(df_selected_pd)\n",
    "\n",
    "# Extract features, target, and weights\n",
    "X = df_selected_pd[feature_cols].astype(np.float32)\n",
    "y = df_selected_pd['responder_6'].astype(np.float32)\n",
    "w = df_selected_pd['weight'].astype(np.float32)\n",
    "\n",
    "# Free up memory by deleting the DataFrame\n",
    "del df_selected_pd\n",
    "gc.collect()\n",
    "\n",
    "# Create DMatrix for training\n",
    "dtrain = xgb.DMatrix(X, label=y, weight=w)\n",
    "\n",
    "# Free up memory by deleting the Pandas DataFrames\n",
    "del X, y, w\n",
    "gc.collect()\n",
    "\n",
    "def weighted_r2_metric(preds, dtrain):\n",
    "    \"\"\"Custom weighted RÂ² evaluation metric for XGBoost.\"\"\"\n",
    "    y_true = dtrain.get_label()\n",
    "    w = dtrain.get_weight()\n",
    "    numerator = np.sum(w * (y_true - preds) ** 2)\n",
    "    denominator = np.sum(w * y_true ** 2)\n",
    "    r2 = 1 - (numerator / denominator)\n",
    "    return 'weighted_r2', r2\n",
    "\n",
    "# Best hyperparameters from your grid search (remove 'n_estimators')\n",
    "best_params = {\n",
    "    'max_depth': 4,\n",
    "    'learning_rate': 0.09996,\n",
    "    'subsample': 0.70679,\n",
    "    'colsample_bytree': 0.73263,\n",
    "    'gamma': 0.26034,\n",
    "    'min_child_weight': 3,\n",
    "    'reg_alpha': 0.43891,\n",
    "    'reg_lambda': 0.59725,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': 'hist',\n",
    "    'eval_metric': 'rmse',  # Base evaluation metric\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Set up MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")  # Update if different\n",
    "mlflow.set_experiment('Jane Street Forecasting Final Model 3')\n",
    "\n",
    "# Disable automatic MLflow logging to customize\n",
    "mlflow.xgboost.autolog(disable=True)\n",
    "\n",
    "# Debugging: Check if necessary variables are defined\n",
    "print(f\"dtrain defined: {'dtrain' in locals() or 'dtrain' in globals()}\")\n",
    "\n",
    "with mlflow.start_run(run_name='Final Model Training on All Data'):\n",
    "    \n",
    "    # Log hyperparameters (excluding 'n_estimators')\n",
    "    mlflow.log_params(best_params)\n",
    "    \n",
    "    # Train the final model without validation\n",
    "    model = xgb.train(\n",
    "        params=best_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=75,  # Set to best_iteration found earlier\n",
    "        custom_metric=weighted_r2_metric,  # Custom metric\n",
    "        maximize=True,  # Because higher RÂ² is better\n",
    "        verbose_eval=10\n",
    "    )\n",
    "    \n",
    "    # Log the best iteration (since we set num_boost_round, it's fixed)\n",
    "    mlflow.log_metric('best_iteration', 75)\n",
    "    \n",
    "    # Save and log the model using joblib\n",
    "    joblib.dump(model, 'final_model.joblib')\n",
    "    mlflow.log_artifact('final_model.joblib')\n",
    "    \n",
    "    # Extract and log feature importance\n",
    "    importance = model.get_score(importance_type='gain')\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': list(importance.keys()),\n",
    "        'Importance': list(importance.values())\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # Optional: Log all feature importances or limit to top N\n",
    "    top_n = 15\n",
    "    importance_df_top = importance_df.head(top_n)\n",
    "    \n",
    "    # Save feature importance to a CSV file\n",
    "    importance_df_top.to_csv('feature_importance.csv', index=False)\n",
    "    mlflow.log_artifact('feature_importance.csv')\n",
    "    \n",
    "    # Plot and log feature importance\n",
    "    plot_filename = plot_feature_importance(importance_df_top, title='Feature Importance - Final Model')\n",
    "    mlflow.log_artifact(plot_filename, artifact_path='feature_importance_plots')\n",
    "    \n",
    "    # Compute and log the custom weighted RÂ² on the entire training set\n",
    "    y_pred = model.predict(dtrain)\n",
    "    # To compute weighted RÂ², you need to access the original labels and weights\n",
    "    # Since we deleted X, y, w earlier, we need to retrieve them from dtrain\n",
    "    y_true = dtrain.get_label()\n",
    "    w = dtrain.get_weight()\n",
    "    weighted_r2 = 1 - (np.sum(w * (y_true - y_pred) ** 2) / np.sum(w * y_true ** 2))\n",
    "    print(f\"Training Weighted RÂ²: {weighted_r2:.4f}\")\n",
    "    mlflow.log_metric('training_weighted_r2', weighted_r2)\n",
    "    \n",
    "    # Compute RMSE for training set\n",
    "    rmse = np.sqrt(np.average((y_true - y_pred) ** 2, weights=w))\n",
    "    print(f\"Training RMSE: {rmse:.4f}\")\n",
    "    mlflow.log_metric('training_rmse', rmse)\n",
    "    \n",
    "    # Residual Analysis\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_pred, residuals, alpha=0.3)\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residuals vs. Predicted Values - Training Set')\n",
    "    plt.tight_layout()\n",
    "    residual_plot = 'residuals_training.png'\n",
    "    plt.savefig(residual_plot)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(residual_plot, artifact_path='residual_plots')\n",
    "    \n",
    "    # Optional: Save feature importance plot in MLflow\n",
    "    # Already done above\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model('customr2_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
