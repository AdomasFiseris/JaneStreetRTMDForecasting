{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import polars as pl\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import glob\n",
    "import pyarrow as pa\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from xgboost import XGBRegressor\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and concatenating the whole dataset\n",
    "\n",
    "class LoadData:\n",
    "    \n",
    "    def __init__(self, file_paths):\n",
    "        self.file_paths = file_paths\n",
    "        \n",
    "    def load_and_concat(self):\n",
    "        # Use `scan_parquet` for lazy loading\n",
    "        partitioned_data = [pl.scan_parquet(file_path) for file_path in self.file_paths]\n",
    "        df = pl.concat(partitioned_data, rechunk=False)  # Keep lazy mode with rechunk=False\n",
    "        \n",
    "        return df\n",
    "    \n",
    "# Specify file paths\n",
    "file_paths = sorted(glob.glob('Data/train.parquet/*/*.parquet'))\n",
    "\n",
    "# Initialize the loader and load data as a lazy frame\n",
    "loader = LoadData(file_paths)\n",
    "df_full = loader.load_and_concat()  # df_train is now a lazy frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering the daily mean value for each responder\n",
    "\n",
    "responder_columns = [col for col, dtype in df_full.schema.items() if col.startswith(\"responder_\")]\n",
    "aggregations = [pl.col(responder).mean().alias(f\"daily_{responder}_mean\") for responder in responder_columns]\n",
    "df_daily_means = df_full.group_by(\"date_id\").agg(aggregations)\n",
    "df_full = df_full.join(df_daily_means, on=\"date_id\")\n",
    "df_full_collected = df_full.collect()\n",
    "df_full_collected['date_id'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the whole dataset into train and validation at 75th percentile of the date_id value\n",
    "\n",
    "train_df_collected = df_full_collected.filter(pl.col('date_id') < (np.percentile(df_full_collected['date_id'].to_numpy(), 75)))\n",
    "val_df_colelcted = df_full_collected.filter(pl.col('date_id') >= (np.percentile(df_full_collected['date_id'].to_numpy(), 75)))\n",
    "\n",
    "len_train = len(train_df_collected)\n",
    "len_valid = len(val_df_colelcted)\n",
    "\n",
    "train_df = train_df_collected.lazy()\n",
    "val_df = val_df_colelcted.lazy()\n",
    "\n",
    "del train_df_collected\n",
    "del val_df_colelcted\n",
    "del df_full_collected\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "%%time\n",
    "\n",
    "# Initialize the model with parameters for incremental learning\n",
    "xgb_model = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 50000\n",
    "\n",
    "# Placeholder for the existing model\n",
    "existing_model = None\n",
    "\n",
    "# Train the model in chunks\n",
    "for i in range(0, len_train, chunk_size):\n",
    "    # Load a chunk of data\n",
    "    chunk = train_df.slice(i, chunk_size).collect().to_pandas()\n",
    "\n",
    "    # Separate features and target\n",
    "    X_chunk = chunk.drop(columns=['responder_6'])\n",
    "    y_chunk = chunk['responder_6']\n",
    "    \n",
    "    # Fit the model incrementally\n",
    "    xgb_model.fit(X_chunk, y_chunk, xgb_model=existing_model)\n",
    "    \n",
    "    # Update the existing model\n",
    "    existing_model = xgb_model.get_booster()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model (if necessary)\n",
    "model = xgb.Booster()\n",
    "model.load_model('xgb_model.json')\n",
    "\n",
    "# Get feature importance scores\n",
    "feature_importance = model.get_score(importance_type='weight')\n",
    "\n",
    "# Convert to a DataFrame for easier handling\n",
    "feature_importance_df = pd.DataFrame(\n",
    "    feature_importance.items(),\n",
    "    columns=['Feature', 'Importance']\n",
    ")\n",
    "\n",
    "# Normalize the importance scores\n",
    "feature_importance_df['Importance'] = feature_importance_df['Importance'] / feature_importance_df['Importance'].sum()\n",
    "\n",
    "# Sort by importance and take the top 20\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False).head(20)\n",
    "\n",
    "# Plot the feature importance for the top 20 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "plt.xlabel('Normalized Importance')\n",
    "plt.title('Top 20 Feature Importance (Normalized)')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to show the most important feature at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
