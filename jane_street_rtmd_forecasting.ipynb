{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afise\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\dask\\dataframe\\__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Packages\n",
    "\n",
    "import os\n",
    "import polars as pl\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import glob\n",
    "import dask.dataframe as dd\n",
    "import pyarrow as pa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from dask.distributed import Client\n",
    "from xgboost import dask as dxgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and concatenating the whole dataset\n",
    "\n",
    "class LoadData:\n",
    "    \n",
    "    def __init__(self, file_paths):\n",
    "        self.file_paths = file_paths\n",
    "        \n",
    "    def load_and_concat(self):\n",
    "        # Use `scan_parquet` for lazy loading\n",
    "        partitioned_data = [pl.scan_parquet(file_path) for file_path in self.file_paths]\n",
    "        df = pl.concat(partitioned_data, rechunk=False)  # Keep lazy mode with rechunk=False\n",
    "        \n",
    "        return df\n",
    "    \n",
    "# Specify file paths\n",
    "file_paths = sorted(glob.glob('Data/train.parquet/*/*.parquet'))\n",
    "\n",
    "# Initialize the loader and load data as a lazy frame\n",
    "loader = LoadData(file_paths)\n",
    "df_full = loader.load_and_concat()  # df_train is now a lazy frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afise\\AppData\\Local\\Temp\\ipykernel_8788\\3960772314.py:3: PerformanceWarning: Resolving the schema of a LazyFrame is a potentially expensive operation. Use `LazyFrame.collect_schema()` to get the schema without this warning.\n",
      "  responder_columns = [col for col, dtype in df_full.schema.items() if col.startswith(\"responder_\")]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>value</th></tr><tr><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>4.7127338e7</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>1005.479389</td></tr><tr><td>&quot;std&quot;</td><td>445.181943</td></tr><tr><td>&quot;min&quot;</td><td>0.0</td></tr><tr><td>&quot;25%&quot;</td><td>679.0</td></tr><tr><td>&quot;50%&quot;</td><td>1060.0</td></tr><tr><td>&quot;75%&quot;</td><td>1376.0</td></tr><tr><td>&quot;max&quot;</td><td>1698.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 2)\n",
       "┌────────────┬─────────────┐\n",
       "│ statistic  ┆ value       │\n",
       "│ ---        ┆ ---         │\n",
       "│ str        ┆ f64         │\n",
       "╞════════════╪═════════════╡\n",
       "│ count      ┆ 4.7127338e7 │\n",
       "│ null_count ┆ 0.0         │\n",
       "│ mean       ┆ 1005.479389 │\n",
       "│ std        ┆ 445.181943  │\n",
       "│ min        ┆ 0.0         │\n",
       "│ 25%        ┆ 679.0       │\n",
       "│ 50%        ┆ 1060.0      │\n",
       "│ 75%        ┆ 1376.0      │\n",
       "│ max        ┆ 1698.0      │\n",
       "└────────────┴─────────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature engineering the daily mean value for each responder\n",
    "\n",
    "responder_columns = [col for col, dtype in df_full.schema.items() if col.startswith(\"responder_\")]\n",
    "aggregations = [pl.col(responder).mean().alias(f\"daily_{responder}_mean\") for responder in responder_columns]\n",
    "df_daily_means = df_full.group_by(\"date_id\").agg(aggregations)\n",
    "df_full = df_full.join(df_daily_means, on=\"date_id\")\n",
    "df_full = df_full.collect()\n",
    "df_full['date_id'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35309026, 101)\n",
      "(11818312, 101)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the whole dataset into train and validation at 75th percentile of the date_id value\n",
    "\n",
    "train_df = df_full.filter(pl.col('date_id') < (np.percentile(df_full['date_id'].to_numpy(), 75)))\n",
    "val_df = df_full.filter(pl.col('date_id') >= (np.percentile(df_full['date_id'].to_numpy(), 75)))\n",
    "\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Train indices: [0 1 2 3 4 5 6 7 8 9] ... [1969712 1969713 1969714 1969715 1969716 1969717 1969718 1969719 1969720\n",
      " 1969721]\n",
      "Test indices: [1969722 1969723 1969724 1969725 1969726 1969727 1969728 1969729 1969730\n",
      " 1969731] ... [3939430 3939431 3939432 3939433 3939434 3939435 3939436 3939437 3939438\n",
      " 3939439]\n",
      "Train set size: 1969722\n",
      "Test set size: 1969718\n",
      "==================================================\n",
      "Fold 2\n",
      "Train indices: [0 1 2 3 4 5 6 7 8 9] ... [3939430 3939431 3939432 3939433 3939434 3939435 3939436 3939437 3939438\n",
      " 3939439]\n",
      "Test indices: [3939440 3939441 3939442 3939443 3939444 3939445 3939446 3939447 3939448\n",
      " 3939449] ... [5909148 5909149 5909150 5909151 5909152 5909153 5909154 5909155 5909156\n",
      " 5909157]\n",
      "Train set size: 3939440\n",
      "Test set size: 1969718\n",
      "==================================================\n",
      "Fold 3\n",
      "Train indices: [0 1 2 3 4 5 6 7 8 9] ... [5909148 5909149 5909150 5909151 5909152 5909153 5909154 5909155 5909156\n",
      " 5909157]\n",
      "Test indices: [5909158 5909159 5909160 5909161 5909162 5909163 5909164 5909165 5909166\n",
      " 5909167] ... [7878866 7878867 7878868 7878869 7878870 7878871 7878872 7878873 7878874\n",
      " 7878875]\n",
      "Train set size: 5909158\n",
      "Test set size: 1969718\n",
      "==================================================\n",
      "Fold 4\n",
      "Train indices: [0 1 2 3 4 5 6 7 8 9] ... [7878866 7878867 7878868 7878869 7878870 7878871 7878872 7878873 7878874\n",
      " 7878875]\n",
      "Test indices: [7878876 7878877 7878878 7878879 7878880 7878881 7878882 7878883 7878884\n",
      " 7878885] ... [9848584 9848585 9848586 9848587 9848588 9848589 9848590 9848591 9848592\n",
      " 9848593]\n",
      "Train set size: 7878876\n",
      "Test set size: 1969718\n",
      "==================================================\n",
      "Fold 5\n",
      "Train indices: [0 1 2 3 4 5 6 7 8 9] ... [9848584 9848585 9848586 9848587 9848588 9848589 9848590 9848591 9848592\n",
      " 9848593]\n",
      "Test indices: [9848594 9848595 9848596 9848597 9848598 9848599 9848600 9848601 9848602\n",
      " 9848603] ... [11818302 11818303 11818304 11818305 11818306 11818307 11818308 11818309\n",
      " 11818310 11818311]\n",
      "Train set size: 9848594\n",
      "Test set size: 1969718\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Convert val_df to pandas DataFrame\n",
    "val_df_pandas = val_df.collect().to_pandas() if isinstance(val_df, pl.LazyFrame) else val_df.to_pandas()\n",
    "\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Print the indices for each fold to understand the structure\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(val_df_pandas), start=1):\n",
    "    print(f\"Fold {fold}\")\n",
    "    print(\"Train indices:\", train_index[:10], \"...\", train_index[-10:])  # Show first & last 10 indices\n",
    "    print(\"Test indices:\", test_index[:10], \"...\", test_index[-10:])    # Show first & last 10 indices\n",
    "    print(\"Train set size:\", len(train_index))\n",
    "    print(\"Test set size:\", len(test_index))\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Training the baseline XGBoost model\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Convert Polars DataFrame to Dask DataFrame\n",
    "train_dask_df = dd.from_pandas(train_df.to_pandas(), npartitions=10)  # Choose a suitable partition size\n",
    "\n",
    "# Define features and target\n",
    "X_train = train_dask_df.drop(columns=['responder_6'])\n",
    "y_train = train_dask_df['responder_6']\n",
    "\n",
    "# Train using XGBoost Dask interface\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"max_depth\": 6,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"n_estimators\": 100,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "xgb_model = dxgb.train(client, params, dtrain=dxgb.DaskDMatrix(client, X_train, y_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
