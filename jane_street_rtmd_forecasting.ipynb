{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import polars as pl\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import warnings\n",
    "from itertools import product\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and concatenating the whole dataset\n",
    "\n",
    "class LoadData:\n",
    "    \n",
    "    def __init__(self, file_paths):\n",
    "        self.file_paths = file_paths\n",
    "        \n",
    "    def load_and_concat(self):\n",
    "\n",
    "        partitioned_data = [pl.read_parquet(file_path) for file_path in self.file_paths]\n",
    "        df = pl.concat(partitioned_data, rechunk=False)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "# Specify file paths\n",
    "file_paths = sorted(glob.glob('Data/train.parquet/*/*.parquet'))\n",
    "\n",
    "# Initialize the loader and load data as a lazy frame\n",
    "loader = LoadData(file_paths)\n",
    "df_full = loader.load_and_concat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Collect unique date_ids\n",
    "\n",
    "unique_date_ids = df_full.select('date_id').unique().sort('date_id')\n",
    "total_days = unique_date_ids.height\n",
    "trading_days_per_year = 252  # Approximate number of trading days per year\n",
    "total_years = total_days / trading_days_per_year\n",
    "train_ratio = 0.9\n",
    "train_days = int(total_days * train_ratio)\n",
    "holdout_days = total_days - train_days\n",
    "training_date_ids = unique_date_ids['date_id'][:train_days]\n",
    "holdout_date_ids = unique_date_ids['date_id'][train_days:]\n",
    "\n",
    "# Training Data (Cross-Validation)\n",
    "train_df_collected = df_full.filter(pl.col('date_id').is_in(training_date_ids))\n",
    "\n",
    "# Hold-Out Data\n",
    "holdout_df_collected = df_full.filter(pl.col('date_id').is_in(holdout_date_ids))\n",
    "\n",
    "print(f\"Total number of unique days: {total_days}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_full = df_full.sort(['date_id', 'time_id'])\n",
    "\n",
    "# Get the indices where each date_id starts\n",
    "date_id_series = df_full['date_id'].to_numpy()\n",
    "unique_date_ids = np.unique(date_id_series)\n",
    "date_id_to_indices = {}\n",
    "\n",
    "for date_id in unique_date_ids:\n",
    "    indices = np.where(date_id_series == date_id)[0]\n",
    "    date_id_to_indices[date_id] = indices\n",
    "\n",
    "# Define the number of folds\n",
    "n_splits = 8\n",
    "\n",
    "# Calculate the number of days per fold\n",
    "days_per_fold = len(unique_date_ids) // n_splits\n",
    "\n",
    "fold_indices = []\n",
    "for i in range(n_splits):\n",
    "    start_day = i * days_per_fold\n",
    "    if i == n_splits - 1:\n",
    "        # Include all remaining days in the last fold\n",
    "        end_day = len(unique_date_ids)\n",
    "    else:\n",
    "        end_day = (i + 1) * days_per_fold\n",
    "    fold_date_ids = unique_date_ids[start_day:end_day]\n",
    "    fold_indices.append(np.concatenate([date_id_to_indices[date_id] for date_id in fold_date_ids]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"Time Series Forecasting Experiment 2\")\n",
    "df_full = df_full.sort(['date_id', 'time_id'])\n",
    "\n",
    "# Define hyperparameter distributions for random search\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 500),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'learning_rate': uniform(0.01, 0.19),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'gamma': uniform(0, 0.5),\n",
    "    'min_child_weight': randint(1, 6),\n",
    "    'reg_alpha': uniform(0.0, 1.0),\n",
    "    'reg_lambda': uniform(0.5, 4.5)\n",
    "}\n",
    "\n",
    "# Number of hyperparameter combinations to try\n",
    "n_iter = 50\n",
    "\n",
    "# Initialize random number generator\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "# Generate random hyperparameter combinations\n",
    "hyperparameter_list = []\n",
    "for _ in range(n_iter):\n",
    "    params = {\n",
    "        'n_estimators': rng.integers(50, 500),\n",
    "        'max_depth': rng.integers(3, 10),\n",
    "        'learning_rate': rng.uniform(0.01, 0.2),\n",
    "        'subsample': rng.uniform(0.6, 1.0),\n",
    "        'colsample_bytree': rng.uniform(0.6, 1.0),\n",
    "        'gamma': rng.uniform(0, 0.5),\n",
    "        'min_child_weight': rng.integers(1, 6),\n",
    "        'reg_alpha': rng.uniform(0.0, 1.0),\n",
    "        'reg_lambda': rng.uniform(0.5, 5.0)\n",
    "    }\n",
    "    hyperparameter_list.append(params)\n",
    "\n",
    "\n",
    "# Exclude non-informative features\n",
    "excluded_features = ['responder_6']\n",
    "\n",
    "# Define feature columns (excluding the target and unwanted features)\n",
    "feature_cols = [col for col in df_full.columns if col not in excluded_features]\n",
    "\n",
    "# Define custom cross-validation folds\n",
    "n_splits = 8\n",
    "data_length = len(df_full)\n",
    "\n",
    "# Split the data into n_splits folds\n",
    "fold_sizes = np.full(n_splits, data_length // n_splits)\n",
    "fold_sizes[:data_length % n_splits] += 1  # Distribute the remainder\n",
    "\n",
    "indices = np.arange(data_length)\n",
    "current = 0\n",
    "fold_indices_list = []\n",
    "for fold_size in fold_sizes:\n",
    "    start, stop = current, current + fold_size\n",
    "    fold_indices_list.append(indices[start:stop])\n",
    "    current = stop\n",
    "\n",
    "# Create custom fold indices\n",
    "validation_fraction = 0.2\n",
    "\n",
    "custom_fold_indices = []\n",
    "\n",
    "for i in range(1, n_splits):\n",
    "    # Get indices for folds i - 1 and i\n",
    "    fold_i_minus1_indices = fold_indices_list[i - 1]\n",
    "    fold_i_indices = fold_indices_list[i]\n",
    "\n",
    "    # Split fold_i_indices into training and validation sets\n",
    "    val_size = int(len(fold_i_indices) * validation_fraction)\n",
    "    if val_size == 0:\n",
    "        val_size = 1  # Ensure at least one sample in validation\n",
    "\n",
    "    train_fold_i_indices = fold_i_indices[:-val_size]\n",
    "    val_fold_i_indices = fold_i_indices[-val_size:]\n",
    "\n",
    "    # Combine training indices\n",
    "    train_indices = np.concatenate([fold_i_minus1_indices, train_fold_i_indices])\n",
    "\n",
    "    # Validation indices\n",
    "    val_indices = val_fold_i_indices\n",
    "\n",
    "    custom_fold_indices.append((train_indices, val_indices))\n",
    "\n",
    "def train_evaluate_model(df_full, feature_cols, params, custom_fold_indices):\n",
    "    import matplotlib.pyplot as plt  # Ensure matplotlib is imported\n",
    "    import pandas as pd             # Import pandas for DataFrame operations\n",
    "    import os                       # For file operations if needed\n",
    "\n",
    "    fold_rmse_list = []\n",
    "    fold_mse_list = []\n",
    "    fold_r2_list = []\n",
    "\n",
    "    for fold, (train_indices, val_indices) in enumerate(custom_fold_indices):\n",
    "        print(f\"\\nProcessing Fold {fold + 1}\")\n",
    "        try:\n",
    "            # Select training and validation data from Polars DataFrame\n",
    "            df_train = df_full[train_indices]\n",
    "            df_val = df_full[val_indices]\n",
    "\n",
    "            # Convert Polars DataFrames to Pandas DataFrames for XGBoost\n",
    "            df_train_pd = df_train.to_pandas()\n",
    "            df_val_pd = df_val.to_pandas()\n",
    "\n",
    "            # Extract features and target\n",
    "            X_train = df_train_pd[feature_cols].astype(np.float32)\n",
    "            y_train = df_train_pd['responder_6'].astype(np.float32)\n",
    "            X_val = df_val_pd[feature_cols].astype(np.float32)\n",
    "            y_val = df_val_pd['responder_6'].astype(np.float32)\n",
    "\n",
    "            # Convert to XGBoost DMatrix\n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "            # Set up model parameters\n",
    "            model_params = params.copy()\n",
    "            model_params.update({\n",
    "                'objective': 'reg:squarederror',\n",
    "                'tree_method': 'hist',     # Efficient tree method\n",
    "                'eval_metric': 'rmse',\n",
    "                'n_jobs': -1,              # Utilize all CPU cores\n",
    "                'verbosity': 0,\n",
    "                'seed': 42                 # For reproducibility\n",
    "            })\n",
    "\n",
    "            # Specify evaluation set\n",
    "            evals = [(dval, 'validation')]\n",
    "\n",
    "            # Train the model with early stopping\n",
    "            model = xgb.train(\n",
    "                model_params,\n",
    "                dtrain,\n",
    "                num_boost_round=1000,\n",
    "                evals=evals,\n",
    "                early_stopping_rounds=10,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "\n",
    "            # Predict and evaluate\n",
    "            y_pred = model.predict(dval)\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "            print(f\"Fold {fold + 1} - RMSE: {rmse:.4f}, MSE: {mse:.4f}, R^2: {r2:.4f}\")\n",
    "\n",
    "            # Store metrics\n",
    "            fold_rmse_list.append(rmse)\n",
    "            fold_mse_list.append(mse)\n",
    "            fold_r2_list.append(r2)\n",
    "\n",
    "            # Log metrics\n",
    "            mlflow.log_metric(f\"rmse_fold_{fold + 1}\", rmse)\n",
    "            mlflow.log_metric(f\"mse_fold_{fold + 1}\", mse)\n",
    "            mlflow.log_metric(f\"r2_fold_{fold + 1}\", r2)\n",
    "\n",
    "            # --------- Feature Importance Code Starts Here ---------\n",
    "\n",
    "            # Extract feature importance\n",
    "            feature_importance = model.get_score(importance_type='gain')\n",
    "            sorted_importance = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "            importance_df = pd.DataFrame(sorted_importance, columns=['Feature', 'Importance'])\n",
    "\n",
    "            # Limit to top 15 features\n",
    "            top_n = 15\n",
    "            importance_df = importance_df.head(top_n)\n",
    "\n",
    "            # Save feature importance to a CSV file\n",
    "            importance_csv = f'feature_importance_fold_{fold + 1}.csv'\n",
    "            importance_df.to_csv(importance_csv, index=False)\n",
    "\n",
    "            # Log the CSV file as an artifact\n",
    "            mlflow.log_artifact(importance_csv, artifact_path=f'feature_importance/fold_{fold + 1}')\n",
    "\n",
    "            # Plot feature importance\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            importance_df.plot(kind='bar', x='Feature', y='Importance', legend=False)\n",
    "            plt.title(f'Feature Importance - Fold {fold + 1}')\n",
    "            plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Save the plot as an image\n",
    "            importance_plot = f'feature_importance_fold_{fold + 1}.png'\n",
    "            plt.savefig(importance_plot)\n",
    "\n",
    "            # Log the image as an artifact\n",
    "            mlflow.log_artifact(importance_plot, artifact_path=f'feature_importance/fold_{fold + 1}')\n",
    "\n",
    "            # Close the plot to free memory\n",
    "            plt.close()\n",
    "\n",
    "            # --------- Feature Importance Code Ends Here ---------\n",
    "\n",
    "            # Clean up\n",
    "            del df_train, df_val, df_train_pd, df_val_pd, X_train, X_val, y_train, y_val, dtrain, dval, model\n",
    "            gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error on fold {fold + 1}: {e}\")\n",
    "            mlflow.log_param(f\"error_fold_{fold + 1}\", str(e))\n",
    "            continue\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_rmse = np.mean(fold_rmse_list)\n",
    "    std_rmse = np.std(fold_rmse_list)\n",
    "    avg_mse = np.mean(fold_mse_list)\n",
    "    std_mse = np.std(fold_mse_list)\n",
    "    avg_r2 = np.mean(fold_r2_list)\n",
    "    std_r2 = np.std(fold_r2_list)\n",
    "\n",
    "    return {\n",
    "        'avg_rmse': avg_rmse,\n",
    "        'std_rmse': std_rmse,\n",
    "        'avg_mse': avg_mse,\n",
    "        'std_mse': std_mse,\n",
    "        'avg_r2': avg_r2,\n",
    "        'std_r2': std_r2\n",
    "    }\n",
    "\n",
    "# Initialize results list\n",
    "grid_search_results = []\n",
    "\n",
    "# Begin MLflow run\n",
    "with mlflow.start_run(run_name=\"Random Hyperparameter Search with Custom Cross-Validation\") as parent_run:\n",
    "    for idx, params in enumerate(hyperparameter_list):\n",
    "        print(f\"\\nEvaluating hyperparameters set {idx + 1}/{n_iter}: {params}\")\n",
    "\n",
    "        # Start a nested MLflow run for this hyperparameter combination\n",
    "        with mlflow.start_run(run_name=f\"Params set {idx + 1}\", nested=True):\n",
    "            # Log hyperparameters\n",
    "            mlflow.log_params(params)\n",
    "\n",
    "            # Train and evaluate the model\n",
    "            metrics = train_evaluate_model(df_full, feature_cols, params, custom_fold_indices)\n",
    "\n",
    "            # Log average metrics\n",
    "            mlflow.log_metric(\"avg_rmse\", metrics['avg_rmse'])\n",
    "            mlflow.log_metric(\"std_rmse\", metrics['std_rmse'])\n",
    "            mlflow.log_metric(\"avg_mse\", metrics['avg_mse'])\n",
    "            mlflow.log_metric(\"std_mse\", metrics['std_mse'])\n",
    "            mlflow.log_metric(\"avg_r2\", metrics['avg_r2'])\n",
    "            mlflow.log_metric(\"std_r2\", metrics['std_r2'])\n",
    "\n",
    "            # Save results\n",
    "            result = {\n",
    "                'params': params,\n",
    "                'avg_rmse': metrics['avg_rmse'],\n",
    "                'std_rmse': metrics['std_rmse'],\n",
    "                'avg_mse': metrics['avg_mse'],\n",
    "                'std_mse': metrics['std_mse'],\n",
    "                'avg_r2': metrics['avg_r2'],\n",
    "                'std_r2': metrics['std_r2']\n",
    "            }\n",
    "            grid_search_results.append(result)\n",
    "\n",
    "            # End of hyperparameter combination run\n",
    "            # The 'with' block takes care of ending the run\n",
    "\n",
    "    # Save grid search results as an artifact\n",
    "\n",
    "    class NumpyEncoder(json.JSONEncoder):\n",
    "        def default(self, obj):\n",
    "            if isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            elif isinstance(obj, (np.bool_)):\n",
    "                return bool(obj)\n",
    "            else:\n",
    "                return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "    # At the end of your script\n",
    "    with open('grid_search_results.json', 'w') as f:\n",
    "        json.dump(grid_search_results, f, indent=4, cls=NumpyEncoder)\n",
    "    mlflow.log_artifact('grid_search_results.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with adjusted hyperparameters\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=10,         # Increased from 5 to 10\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,       # Increased from 0.01 to 0.1\n",
    "    reg_alpha=0.1,           # L1 regularization\n",
    "    reg_lambda=1.0,          # L2 regularization\n",
    "    random_state=42,\n",
    "    verbosity=1\n",
    ")\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 1000000  # Increased from 50,000 to 1,000,000\n",
    "\n",
    "# Placeholder for the existing booster\n",
    "existing_model = None\n",
    "\n",
    "# Placeholder for feature columns\n",
    "feature_columns = None\n",
    "\n",
    "# Optional: Shuffle the dataset before training to ensure representative chunks\n",
    "# Uncomment the following lines if your dataset isn't already shuffled\n",
    "# train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Starting incremental training...\")\n",
    "for i in tqdm(range(0, len_train, chunk_size), desc=\"Training Progress\", leave=True):\n",
    "    # Load and process a chunk of data\n",
    "    chunk = train_df.slice(i, chunk_size).collect().to_pandas()\n",
    "    X_chunk = chunk.drop(columns=['responder_6'])\n",
    "    y_chunk = chunk['responder_6']\n",
    "    \n",
    "    # Save feature columns from the first chunk\n",
    "    if i == 0:\n",
    "        feature_columns = X_chunk.columns.tolist()\n",
    "        with open(\"feature_columns.pkl\", \"wb\") as f:\n",
    "            pickle.dump(feature_columns, f)\n",
    "    \n",
    "    # Ensure feature alignment\n",
    "    X_chunk = X_chunk[feature_columns]\n",
    "    \n",
    "    # Define a small validation set from the current chunk\n",
    "    # Here, we take 10% of the chunk as a temporary validation set\n",
    "    X_val = X_chunk.sample(frac=0.1, random_state=42)\n",
    "    y_val = y_chunk.loc[X_val.index]\n",
    "    \n",
    "    # Remove validation samples from training data\n",
    "    X_train_chunk = X_chunk.drop(index=X_val.index)\n",
    "    y_train_chunk = y_chunk.drop(index=X_val.index)\n",
    "    \n",
    "    # Fit the model incrementally with early stopping\n",
    "    xgb_model.fit(\n",
    "        X_train_chunk, \n",
    "        y_train_chunk, \n",
    "        xgb_model=existing_model, \n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Update the existing booster\n",
    "    existing_model = xgb_model.get_booster()\n",
    "    \n",
    "    # Monitor total number of trees\n",
    "    total_trees = xgb_model.get_booster().trees_to_dataframe().shape[0]\n",
    "    print(f\"Total number of trees after this chunk: {total_trees}\")\n",
    "    \n",
    "    # Sample prediction for monitoring\n",
    "    sample_pred = xgb_model.predict(X_train_chunk.sample(5))\n",
    "    print(f\"Sample Predictions after this chunk: {sample_pred}\")\n",
    "    \n",
    "    # Stop training if total trees exceed 10,000 (adjust as needed)\n",
    "    if total_trees >= 100000:\n",
    "        print(\"Reached the maximum number of trees. Stopping training.\")\n",
    "        break\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Save the entire model using joblib\n",
    "joblib.dump(xgb_model, \"xgb_baseline_model.joblib\")\n",
    "print(\"Model saved using joblib.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
