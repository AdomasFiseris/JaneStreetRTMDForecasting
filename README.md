# JaneÂ Streetâ€¯â€¢â€¯Realâ€‘Time Market Data Forecasting  
_Kaggle Code CompetitionÂ Â (Octâ€‘2024Â â†’Â Julâ€‘2025)_

Forecasting **`responder_6`**â€”an anonymised marketâ€‘microâ€‘structure returnâ€”using 79 realâ€‘world features streamed from JaneÂ Streetâ€™s production trading flow.  
The task reflects genuine quantâ€‘trading hurdles: fatâ€‘tailed returns, nonâ€‘stationary signals, and frequent regime shifts.

---

## 1â€‚Competition Snapshot
| Item | Detail |
|------|--------|
| **Dataset** | 79 anonymised features Â· 9 responders Â· â‰ˆâ€¯4.5â€¯M rows / phase |
| **Objective** | Predict `responder_6` one step ahead in real time |
| **Metric** | Sampleâ€‘weighted, zeroâ€‘mean ð‘Â² |
| **Rules** | Kaggle notebook submissions â‰¤â€¯8â€¯h (training) / 9â€¯h (forecasting) runtime; external public data permitted |

---

## 2â€‚Modelling Approach

| Layer | Choices & Rationale |
|-------|---------------------|
| **Feature Engineering** | Minimal. Used raw 79 features; optional oneâ€‘day lags for all responders. Extensive engineering risked amplifying concept drift and exceeding memory limits. |
| **Validation Scheme** | Chronological splitâ€”lastÂ 100â€¯days held outâ€”to mimic leaderboard evaluation faithfully. |
| **Custom Metric** | Implemented the official weightedâ€‘RÂ² inside XGBoost and PyTorch training loops for early stopping and model selection. |
| **Experiment Tracking** | MLflow logs hyperâ€‘parameters, metrics, artefacts (feature importances, residual plots, models). |

### 2.1â€‚Gradientâ€‘Boosted Trees (XGBoost)
* Histogram optimiser (`tree_method="hist"`).  
* Key hyperâ€‘parameters (Optunaâ€‘tuned):  

  | `max_depth` | `learning_rate` | `subsample` | `colsample_bytree` | `gamma` |
  |-------------|-----------------|-------------|--------------------|---------|
  | 4 | 0.10 | 0.71 | 0.73 | 0.26 |

* Trained forâ€¯75 rounds with early stopping on validation weightedâ€‘RÂ².

### 2.2â€‚Twoâ€‘Layer LSTM
* **Input**Â : standardâ€‘scaled features (+ optional lag columns).  
* **Architecture**Â : `[LSTM (32Â hidden)Â Ã—Â 2 â†’ FCÂ 1]`, dropoutÂ 0.4, seqâ€‘lengthÂ 1.  
* **Loss**Â : sampleâ€‘weighted MSE.  
* **Optimiser**Â : AdamÂ (lrâ€¯5â€¯Ã—â€¯10â»âµ, weight_decayâ€¯1â€¯Ã—â€¯10â»Â³) with early stopping on training weightedâ€‘RÂ².

---

## 3â€‚Submission Scores

| NotebookÂ /Â Version | Model | PublicÂ LB | PrivateÂ LB |
|--------------------|-------|-----------|------------|
| **Janeâ€¯Streetâ€¯RTDFÂ GBDTÂ â€“Â SubmissionÂ NBâ€¯3Â v1** | XGBoostÂ (75Â trees) | **0.006171** | 0.006171 |
| Best Model LSTMÂ NNâ€¯v1 | Twoâ€‘Layer LSTM | 0.004329 | 0.004329 |
| JSâ€¯LSTMâ€¯1.0Â withâ€¯Lagsâ€¯v2 | Twoâ€‘Layer LSTMÂ (+Â lag inputs) | â€“0.000567 | â€“0.000567 |
| Janeâ€¯Streetâ€¯RTDFÂ GBDTÂ â€“Â SubmissionÂ NBâ€¯2Â v1 | XGBoostÂ (early hyperâ€‘param set) | 0.005870 | 0.005870 |
| Baseline `submission.parquet` | Simple tree baseline | 0.002646 | 0.002646 |
| Janeâ€¯Streetâ€¯RTDF Submission byâ€¯A.F.â€¯v2 | First public attempt | 0.001192 | 0.001192 |

> *Scores shown are those displayed on the public and private leaderboards immediately after each submission.*

---

## 4â€‚Key Learnings
* **Trees outperform RNNs** when sequence context is short and feature interactions dominate.  
* **Temporal CV matters**: random folds overstated performance by >30â€¯%.  
* **Custom metrics inside training loops** reduce leakage between offline validation and Kaggle evaluation.  
* **Memory is scarce** in notebook environments; efficient parquet partition loading (Polars) and onâ€‘theâ€‘fly downâ€‘casting are essential.  

---

## 5â€‚Next Directions
1. Blend XGBoost and LSTM predictions for regime diversification.  
2. Introduce rollingâ€‘window hyperâ€‘parameter tuning to adapt to drift.  
3. Explore symbol embeddings to capture crossâ€‘sectional relationships.  
4. Prune lowâ€‘impact features (via SHAP) to streamline inference.

---

## 6â€‚Acknowledgements
Many thanks to **JaneÂ Street** for releasing a realistic dataset and to the Kaggle community for vibrant discussion threads.

*Last updatedÂ : 20â€¯Julâ€¯2025*
